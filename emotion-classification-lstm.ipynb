{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3c1e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from tokenizers import Tokenizer, models, trainers\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf055df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"dair-ai/emotion\", \"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd9eddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e289c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset[\"train\"]\n",
    "validation_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88b38c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "vocab_n = 5000\n",
    "sequence_len = 64\n",
    "\n",
    "# Initialize a tokenizer using BPE (Byte Pair Encoding)\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.enable_padding(length=sequence_len)\n",
    "tokenizer.enable_truncation(max_length=sequence_len)\n",
    "tokenizer_trainer = trainers.BpeTrainer(vocab_size=vocab_n)\n",
    "tokenizer.train_from_iterator(train_data[\"text\"], trainer=tokenizer_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1463b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, tokenizer: Tokenizer):\n",
    "    \"\"\" \n",
    "    Helper function to tokenize text and return corresponding token IDs as tensors.\n",
    "\n",
    "    Args:\n",
    "        text, str: Text instance from training data.\n",
    "        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.\n",
    "    Returns:\n",
    "        Tensor: One-dimensional PyTorch tensor with token IDs.\n",
    "    \"\"\"\n",
    "    return torch.tensor(tokenizer.encode(text).ids)\n",
    "\n",
    "\n",
    "def preprocess_label(label: int):\n",
    "    \"\"\" \n",
    "    Helper function to return label as tensor.\n",
    "\n",
    "    Args:\n",
    "        label, int: Label from instance.\n",
    "    Returns:\n",
    "        Tensor: One-dimensional PyTorch tensor containing the label index.\n",
    "    \"\"\"\n",
    "    return torch.tensor(label)\n",
    "\n",
    "\n",
    "def preprocess(data: dict, tokenizer: Tokenizer):\n",
    "    \"\"\" \n",
    "    Transforms input dataset to tokenized vector representations.\n",
    "\n",
    "    Args:\n",
    "        data, dict: Dictionary with text instances and labels.\n",
    "        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.\n",
    "    Returns:\n",
    "        list: List with tensors for the input texts and labels.\n",
    "    \"\"\"\n",
    "    instances = []\n",
    "\n",
    "    for text, label in zip(data[\"text\"], data[\"label\"]):\n",
    "        input = preprocess_text(text, tokenizer)\n",
    "        label = preprocess_label(label)\n",
    "        \n",
    "        instances.append((input, label))\n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "392f3974",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instances = preprocess(train_data, tokenizer)\n",
    "val_instances = preprocess(validation_data, tokenizer)\n",
    "test_instances = preprocess(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bab49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching for LSTM input\n",
    "\n",
    "def batching_lstm(instances: list, batch_size: int, shuffle: bool):\n",
    "    \"\"\"\n",
    "    Batching for LSTM input: with padding support.\n",
    "\n",
    "    Args:\n",
    "        instances: List of (input_tensor, label_tensor) pairs.\n",
    "        batch_size: Number of instances per batch.\n",
    "        shuffle: Whether to shuffle the dataset before batching.\n",
    "    \n",
    "    Returns:\n",
    "        batches: List of (padded_input_tensor, label_tensor) for each batch.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        random.shuffle(instances)\n",
    "\n",
    "    batches = []\n",
    "\n",
    "    for i in range(0, len(instances), batch_size):\n",
    "        batch = instances[i : i + batch_size]\n",
    "\n",
    "        # 取出一批 input 和 label\n",
    "        batch_inputs = [item[0] for item in batch]  # list of tensors (seq_len,)\n",
    "        batch_labels = torch.stack([item[1] for item in batch])  # tensor of shape [batch_size]\n",
    "\n",
    "        # 自动 padding，变成 [batch_size, max_seq_len]\n",
    "        padded_inputs = pad_sequence(batch_inputs, batch_first=True, padding_value=0)\n",
    "\n",
    "        batches.append((padded_inputs, batch_labels))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ffb1f",
   "metadata": {},
   "source": [
    "<h2>BiLSTM Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fdb8041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, padding_idx):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        # Word embedding layer, randomly initialized\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=padding_idx)\n",
    "\n",
    "        # Single-layer bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Fully connected layer, output 6 types of emotions\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "        output, (hidden, _) = self.lstm(embedded)  # hidden shape: [2, batch, hidden_dim]\n",
    "\n",
    "        # Take the last hidden layer of the forward and reverse directions and concatenate them\n",
    "        hidden_forward = hidden[-2, :, :]  # [batch, hidden_dim]\n",
    "        hidden_backward = hidden[-1, :, :]  # [batch, hidden_dim]\n",
    "        combined = torch.cat((hidden_forward, hidden_backward), dim=1)  # [batch, hidden_dim * 2]\n",
    "\n",
    "        out = self.dropout(combined)\n",
    "        return self.fc(out)  # Output shape: [batch_size, output_dim]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27d930ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocabulary dictionary from the tokenizer (word → ID)\n",
    "word2idx = tokenizer.get_vocab()  # e.g., {'i': 4, 'love': 5, 'this': 6, ...}\n",
    "\n",
    "# Reversal\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "padding_idx = word2idx.get(\"[PAD]\", 0) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7932da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def train_and_evaluate(model, train_batches, val_batches, num_epochs=5, lr=1e-3, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for batch_x, batch_y in train_batches:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)  # shape: [batch_size, 6]\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "        train_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "        # ---- verify ----\n",
    "        model.eval()\n",
    "        val_preds, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for val_x, val_y in val_batches:\n",
    "                val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "                val_out = model(val_x)\n",
    "                val_pred = torch.argmax(val_out, dim=1)\n",
    "                val_preds.extend(val_pred.cpu().numpy())\n",
    "                val_labels.extend(val_y.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {sum(train_losses)/len(train_losses):.4f} | \"\n",
    "              f\"Train Acc: {train_acc:.4f} | F1: {train_f1:.4f} || \"\n",
    "              f\"Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2415f0f1",
   "metadata": {},
   "source": [
    "<h2>Parameter tuning:</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1cf92fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters tuning\n",
    "model = LSTMClassifier(\n",
    "    vocab_size=len(word2idx),        # vocabulary size\n",
    "    embedding_dim=100,               # Dimensions of word vectors\n",
    "    hidden_dim=128,                  # Hidden layer dimensions\n",
    "    output_dim=6,                    # The number of output categories\n",
    "    padding_idx=word2idx.get(\"[PAD]\", 0)  # Index of pad token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d13b95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 1.4776 | Train Acc: 0.4162 | F1: 0.3440 || Val Acc: 0.5655 | Val F1: 0.4993\n",
      "Epoch 2/10 | Train Loss: 1.2783 | Train Acc: 0.5262 | F1: 0.4613 || Val Acc: 0.6495 | Val F1: 0.5921\n",
      "Epoch 3/10 | Train Loss: 0.8090 | Train Acc: 0.7169 | F1: 0.6723 || Val Acc: 0.7365 | Val F1: 0.6878\n",
      "Epoch 4/10 | Train Loss: 0.5555 | Train Acc: 0.8033 | F1: 0.7837 || Val Acc: 0.7970 | Val F1: 0.7784\n",
      "Epoch 5/10 | Train Loss: 0.3829 | Train Acc: 0.8645 | F1: 0.8620 || Val Acc: 0.8545 | Val F1: 0.8516\n",
      "Epoch 6/10 | Train Loss: 0.2676 | Train Acc: 0.9077 | F1: 0.9073 || Val Acc: 0.8750 | Val F1: 0.8739\n",
      "Epoch 7/10 | Train Loss: 0.1948 | Train Acc: 0.9316 | F1: 0.9316 || Val Acc: 0.8875 | Val F1: 0.8865\n",
      "Epoch 8/10 | Train Loss: 0.1520 | Train Acc: 0.9466 | F1: 0.9465 || Val Acc: 0.8900 | Val F1: 0.8897\n",
      "Epoch 9/10 | Train Loss: 0.1178 | Train Acc: 0.9586 | F1: 0.9585 || Val Acc: 0.8895 | Val F1: 0.8900\n",
      "Epoch 10/10 | Train Loss: 0.1021 | Train Acc: 0.9632 | F1: 0.9632 || Val Acc: 0.8895 | Val F1: 0.8892\n"
     ]
    }
   ],
   "source": [
    "train_instances = preprocess(dataset[\"train\"], tokenizer)\n",
    "val_instances = preprocess(dataset[\"validation\"], tokenizer)\n",
    "\n",
    "train_batches = batching_lstm(train_instances, batch_size=32, shuffle=True)\n",
    "val_batches = batching_lstm(val_instances, batch_size=32, shuffle=False)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_and_evaluate(model, train_batches, val_batches, num_epochs=10, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "afb75b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.94      0.92      0.93       550\n",
      "         joy       0.89      0.93      0.91       704\n",
      "        love       0.76      0.76      0.76       178\n",
      "       anger       0.89      0.86      0.88       275\n",
      "        fear       0.90      0.84      0.87       212\n",
      "    surprise       0.84      0.80      0.82        81\n",
      "\n",
      "    accuracy                           0.89      2000\n",
      "   macro avg       0.87      0.85      0.86      2000\n",
      "weighted avg       0.89      0.89      0.89      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get the label name order\n",
    "label_names = dataset[\"train\"].features[\"label\"].names  # ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "\n",
    "# Model prediction validation set\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in val_batches:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model(x_batch)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        true_labels.extend(y_batch.cpu().numpy())\n",
    "        pred_labels.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Print each category's precision, recall, f1-score\n",
    "print(classification_report(true_labels, pred_labels, target_names=label_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b070648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported: True\n",
      "CUDA version: 11.8\n",
      "Current device: NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA is supported:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Current device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"无\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
