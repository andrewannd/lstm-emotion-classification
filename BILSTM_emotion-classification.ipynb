{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "707d12e0",
   "metadata": {},
   "source": [
    "## Emotion Classification Project - Based on BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3c1e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from tokenizers import Tokenizer, models, trainers\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9895e1c5",
   "metadata": {},
   "source": [
    "## 1. Load sentiment classification dataset\n",
    "Use `datasets` library to load public sentiment classification dataset. This dataset contains 6 sentiment labels and has been classified into training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf055df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"dair-ai/emotion\", \"split\")\n",
    "train_data = dataset[\"train\"]\n",
    "validation_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fd9eddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db376fd0",
   "metadata": {},
   "source": [
    "## 2. Text preprocessing (Tokenization)\n",
    "In order to input text into the neural network, the original text needs to be converted into a sequence of digital IDs through a `tokenizer`. Define the preprocessing function and process the training data in batches.\n",
    "\n",
    "Reference: [Omseeth's Blog - CNN Sentiment Analysis (2024)](https://omseeth.github.io/blog/2024/CNN_sentiment_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88b38c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "vocab_n = 5000\n",
    "sequence_len = 64\n",
    "\n",
    "# Initialize a tokenizer using BPE (Byte Pair Encoding)\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.enable_padding(length=sequence_len)\n",
    "tokenizer.enable_truncation(max_length=sequence_len)\n",
    "tokenizer_trainer = trainers.BpeTrainer(vocab_size=vocab_n)\n",
    "tokenizer.train_from_iterator(train_data[\"text\"], trainer=tokenizer_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f1463b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, tokenizer: Tokenizer):\n",
    "    return torch.tensor(tokenizer.encode(text).ids)\n",
    "\n",
    "\n",
    "def preprocess_label(label: int):\n",
    "    return torch.tensor(label)\n",
    "\n",
    "\n",
    "def preprocess(data: dict, tokenizer: Tokenizer):\n",
    "    instances = []\n",
    "\n",
    "    for text, label in zip(data[\"text\"], data[\"label\"]):\n",
    "        input = preprocess_text(text, tokenizer)\n",
    "        label = preprocess_label(label)\n",
    "        \n",
    "        instances.append((input, label))\n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "392f3974",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instances = preprocess(train_data, tokenizer)\n",
    "val_instances = preprocess(validation_data, tokenizer)\n",
    "test_instances = preprocess(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538c1c3c",
   "metadata": {},
   "source": [
    "## 3. Batching\n",
    "Since the BILSTM model requires batch input of data, the processed data is grouped (batching) to unify the input size of each batch for efficient training.\n",
    "\n",
    "Reference: [Omseeth's Blog - CNN Sentiment Analysis (2024)](https://omseeth.github.io/blog/2024/CNN_sentiment_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9bab49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching for LSTM input\n",
    "\n",
    "def batching_lstm(instances: list, batch_size: int, shuffle: bool):\n",
    "    if shuffle:\n",
    "        random.shuffle(instances)\n",
    "\n",
    "    batches = []\n",
    "\n",
    "    for i in range(0, len(instances), batch_size):\n",
    "        batch = instances[i : i + batch_size]\n",
    "\n",
    "        # Take out a batch of input and label\n",
    "        batch_inputs = [item[0] for item in batch]  # list of tensors (seq_len,)\n",
    "        batch_labels = torch.stack([item[1] for item in batch])  # tensor of shape [batch_size]\n",
    "\n",
    "        # Automatic padding, becomes [batch_size, max_seq_len]\n",
    "        padded_inputs = pad_sequence(batch_inputs, batch_first=True, padding_value=0)\n",
    "\n",
    "        batches.append((padded_inputs, batch_labels))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ffb1f",
   "metadata": {},
   "source": [
    "## 4. BILSTM emotional classification model construction\n",
    "\n",
    "In this project, we built a simple `BILSTM (Bidirectional Long Short-Term Memory Network)` to perform text sentiment classification.\n",
    "\n",
    "### Model structure:\n",
    "\n",
    "- **Input layer**: Receive the text sequence (word IDs) encoded by Tokenizer.\n",
    "\n",
    "- **Embedding layer**: Map each word ID to a fixed-dimensional word vector to learn the semantic representation of the word.\n",
    "\n",
    "- **BILSTM layer**: Extract the temporal features in the text sequence and capture the contextual dependency information.\n",
    "\n",
    "- **Fully connected layer**: Map the output of LSTM to the sentiment category space.\n",
    "\n",
    "- **Output layer (Softmax Activation)**: Output the predicted probability of each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2fdb8041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, padding_idx):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        # Word embedding layer, randomly initialized\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=padding_idx)\n",
    "\n",
    "        # Single-layer bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Fully connected layer, output 6 types of emotions\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "        output, (hidden, _) = self.lstm(embedded)  # hidden shape: [2, batch, hidden_dim]\n",
    "\n",
    "        # Take the last hidden layer of the forward and reverse directions and concatenate them\n",
    "        hidden_forward = hidden[-2, :, :]  # [batch, hidden_dim]\n",
    "        hidden_backward = hidden[-1, :, :]  # [batch, hidden_dim]\n",
    "        combined = torch.cat((hidden_forward, hidden_backward), dim=1)  # [batch, hidden_dim * 2]\n",
    "\n",
    "        out = self.dropout(combined)\n",
    "        return self.fc(out)  # Output shape: [batch_size, output_dim]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "27d930ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocabulary dictionary from the tokenizer (word â†’ ID)\n",
    "word2idx = tokenizer.get_vocab()  # e.g., {'i': 4, 'love': 5, 'this': 6, ...}\n",
    "\n",
    "# Reversal\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "padding_idx = word2idx.get(\"[PAD]\", 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac06e7d8",
   "metadata": {},
   "source": [
    "## 5. Training and Validation Loop\n",
    "\n",
    "- **Model training**: adjust the model to training mode `model.train()`, and use the training set to optimize the parameters in rounds\n",
    "\n",
    "- **Model validation**: adjust the model to validation mode `model.eval()`. After each round, test the performance on the validation set but do not update the model parameters\n",
    "\n",
    "- **Record and print results**: print the Train Loss, Train Accuracy, Train F1, Val Accuracy, Val F1 of each round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7932da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def train_and_evaluate(model, train_batches, val_batches, num_epochs=5, lr=1e-3, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for batch_x, batch_y in train_batches:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)  # shape: [batch_size, 6]\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "        train_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "        # ---- verify ----\n",
    "        model.eval()\n",
    "        val_preds, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for val_x, val_y in val_batches:\n",
    "                val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "                val_out = model(val_x)\n",
    "                val_pred = torch.argmax(val_out, dim=1)\n",
    "                val_preds.extend(val_pred.cpu().numpy())\n",
    "                val_labels.extend(val_y.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {sum(train_losses)/len(train_losses):.4f} | \"\n",
    "              f\"Train Acc: {train_acc:.4f} | F1: {train_f1:.4f} || \"\n",
    "              f\"Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2415f0f1",
   "metadata": {},
   "source": [
    "## 6. Parameter Tuning\n",
    "Try different hyperparameters and find the best configuration by comparing the model performance under different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1cf92fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters tuning\n",
    "model = LSTMClassifier(\n",
    "    vocab_size=len(word2idx),        # vocabulary size\n",
    "    embedding_dim=100,               # Dimensions of word vectors\n",
    "    hidden_dim=128,                  # Hidden layer dimensions\n",
    "    output_dim=6,                    # The number of output categories\n",
    "    padding_idx=word2idx.get(\"[PAD]\", 0)  # Index of pad token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9d13b95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 1.4541 | Train Acc: 0.4280 | F1: 0.3565 || Val Acc: 0.6005 | Val F1: 0.5405\n",
      "Epoch 2/10 | Train Loss: 0.8679 | Train Acc: 0.7076 | F1: 0.6702 || Val Acc: 0.7635 | Val F1: 0.7425\n",
      "Epoch 3/10 | Train Loss: 0.5047 | Train Acc: 0.8297 | F1: 0.8215 || Val Acc: 0.8255 | Val F1: 0.8238\n",
      "Epoch 4/10 | Train Loss: 0.3343 | Train Acc: 0.8871 | F1: 0.8857 || Val Acc: 0.8525 | Val F1: 0.8561\n",
      "Epoch 5/10 | Train Loss: 0.2218 | Train Acc: 0.9253 | F1: 0.9248 || Val Acc: 0.8690 | Val F1: 0.8715\n",
      "Epoch 6/10 | Train Loss: 0.1598 | Train Acc: 0.9464 | F1: 0.9462 || Val Acc: 0.8850 | Val F1: 0.8861\n",
      "Epoch 7/10 | Train Loss: 0.1281 | Train Acc: 0.9561 | F1: 0.9560 || Val Acc: 0.8610 | Val F1: 0.8675\n",
      "Epoch 8/10 | Train Loss: 0.1004 | Train Acc: 0.9649 | F1: 0.9648 || Val Acc: 0.8875 | Val F1: 0.8880\n",
      "Epoch 9/10 | Train Loss: 0.0932 | Train Acc: 0.9688 | F1: 0.9687 || Val Acc: 0.8790 | Val F1: 0.8797\n",
      "Epoch 10/10 | Train Loss: 0.0833 | Train Acc: 0.9711 | F1: 0.9710 || Val Acc: 0.8980 | Val F1: 0.8974\n"
     ]
    }
   ],
   "source": [
    "train_instances = preprocess(dataset[\"train\"], tokenizer)\n",
    "val_instances = preprocess(dataset[\"validation\"], tokenizer)\n",
    "\n",
    "train_batches = batching_lstm(train_instances, batch_size=32, shuffle=True)\n",
    "val_batches = batching_lstm(val_instances, batch_size=32, shuffle=False)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_and_evaluate(model, train_batches, val_batches, num_epochs=10, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "afb75b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.92      0.93      0.92       550\n",
      "         joy       0.93      0.93      0.93       704\n",
      "        love       0.84      0.72      0.78       178\n",
      "       anger       0.88      0.88      0.88       275\n",
      "        fear       0.85      0.89      0.87       212\n",
      "    surprise       0.80      0.84      0.82        81\n",
      "\n",
      "    accuracy                           0.90      2000\n",
      "   macro avg       0.87      0.87      0.87      2000\n",
      "weighted avg       0.90      0.90      0.90      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get the label name order\n",
    "label_names = dataset[\"train\"].features[\"label\"].names  # ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "\n",
    "# Model prediction validation set\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in val_batches:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model(x_batch)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        true_labels.extend(y_batch.cpu().numpy())\n",
    "        pred_labels.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Print each category's precision, recall, f1-score\n",
    "print(classification_report(true_labels, pred_labels, target_names=label_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a97b20",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [1] Omseeth's Blog - CNN Sentiment Analysis (2024). https://omseeth.github.io/blog/2024/CNN_sentiment_analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
