{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0baf3ccf",
   "metadata": {},
   "source": [
    "# CNN + Lemmatization\n",
    "In this notebook, we use the same model from CNN.ipynb but with lemmatization method for data preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d018716b-0cb0-43f3-abfd-ace6f613534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import string\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from tokenizers import Tokenizer, models, trainers\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b4156b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/deyuq/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/deyuq/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e5d4d27-6fcc-4e38-ad99-5aedad8842fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = load_dataset(\"dair-ai/emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0315fef-7e42-4278-8dd5-6078ec8b754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c66816cb-e29d-4940-95bc-b785747447e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = emotions[\"train\"]\n",
    "validation_data = emotions[\"validation\"]\n",
    "test_data = emotions[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602a969",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "Define function for applying lemmatization to training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "451f796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    #remove punctuations and uppercase\n",
    "    clean_text = text.translate(str.maketrans('','',string.punctuation)).lower()\n",
    "    \n",
    "    #remove stopwords\n",
    "    clean_text = [word for word in clean_text.split() if word not in stopwords.words('english')]\n",
    "    \n",
    "    #lemmatize the word\n",
    "    sentence = []\n",
    "    for word in clean_text:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        sentence.append(lemmatizer.lemmatize(word, 'v'))\n",
    "\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "566b323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_train_data = []\n",
    "lemmatized_val_data = []\n",
    "lemmatized_test_data = []\n",
    "for sentence in train_data[\"text\"]:\n",
    "    lemmatized_train_data.append(cleaning(sentence))\n",
    "for sentence in validation_data[\"text\"]:\n",
    "    lemmatized_val_data.append(cleaning(sentence))\n",
    "for sentence in test_data[\"text\"]:\n",
    "    lemmatized_test_data.append(cleaning(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25b2a4d5-ea1c-4196-844c-fddd2a6f2259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "vocab_n = 5000\n",
    "sequence_len = 64\n",
    "\n",
    "# Initialize a tokenizer using BPE (Byte Pair Encoding)\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.enable_padding(length=sequence_len)\n",
    "tokenizer.enable_truncation(max_length=sequence_len)\n",
    "tokenizer_trainer = trainers.BpeTrainer(vocab_size=vocab_n)\n",
    "tokenizer.train_from_iterator(train_data[\"text\"], trainer=tokenizer_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8b3d861-7e4f-48d3-b643-06e9b9a88405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, tokenizer: Tokenizer):\n",
    "    \"\"\" \n",
    "    Helper function to tokenize text and return corresponding token IDs as tensors.\n",
    "\n",
    "    Args:\n",
    "        text, str: Text instance from training data.\n",
    "        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.\n",
    "    Returns:\n",
    "        Tensor: One-dimensional PyTorch tensor with token IDs.\n",
    "    \"\"\"\n",
    "    return torch.tensor(tokenizer.encode(text).ids)\n",
    "\n",
    "\n",
    "def preprocess_label(label: int):\n",
    "    \"\"\" \n",
    "    Helper function to return label as tensor.\n",
    "\n",
    "    Args:\n",
    "        label, int: Label from instance.\n",
    "    Returns:\n",
    "        Tensor: One-dimensional PyTorch tensor containing the label index.\n",
    "    \"\"\"\n",
    "    return torch.tensor(label)\n",
    "\n",
    "\n",
    "def preprocess(data: list, labels: list, tokenizer: Tokenizer):\n",
    "    \"\"\" \n",
    "    Transforms input dataset to tokenized vector representations.\n",
    "\n",
    "    Args:\n",
    "        data, dict: Dictionary with text instances and labels.\n",
    "        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.\n",
    "    Returns:\n",
    "        list: List with tensors for the input texts and labels.\n",
    "    \"\"\"\n",
    "    instances = []\n",
    "\n",
    "    for text, label in zip(data, labels):\n",
    "        input = preprocess_text(text, tokenizer)\n",
    "        label = preprocess_label(label)\n",
    "        \n",
    "        instances.append((input, label))\n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "85779c8a-cb93-472c-8e61-8e7aadf46fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instances = preprocess(lemmatized_train_data, train_data[\"label\"], tokenizer)\n",
    "val_instances = preprocess(lemmatized_val_data, validation_data[\"label\"], tokenizer)\n",
    "test_instances = preprocess(lemmatized_test_data, test_data[\"label\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba01debf-56ad-4302-aa85-8b506e686a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching\n",
    "\n",
    "def batching(instances: list, batch_size: int, shuffle: bool):\n",
    "    \"\"\" \n",
    "    Batches input instances along the given size and returns list of batches.\n",
    "\n",
    "    Args:\n",
    "        instances, list: List of instances, containing a tuple of two tensors \n",
    "            for each text as well as corresponding label.\n",
    "        batch_size, int: Size for batches.\n",
    "        shuffle, bool: If true, the instances will be shuffled before batching.\n",
    "    Returns:\n",
    "        list: List containing tuples that correspond to single batches.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        random.shuffle(instances)\n",
    "\n",
    "    batches = []\n",
    "\n",
    "    # We iterate through the instances with batch_size steps\n",
    "    for i in range(0, len(instances), batch_size):\n",
    "\n",
    "        # Stacking the instances with dim=0 (default value)\n",
    "        batch_texts = torch.stack(\n",
    "            [instance[0] for instance in instances[i : i + batch_size]]\n",
    "        )\n",
    "        batch_labels = torch.stack(\n",
    "            [instance[1] for instance in instances[i : i + batch_size]]\n",
    "        )\n",
    "\n",
    "        batches.append((batch_texts, batch_labels))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5aa7e1e-9735-4c8d-8761-68d251b2036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Network\n",
    "\n",
    "class CNN_Classifier(nn.Module):\n",
    "    \"\"\" \n",
    "    CNN for sentiment classification with 6 classes, consisting of an embedding \n",
    "    layer, two convolutional layers with different filter sizes, different \n",
    "    pooling sizes, as well as one linear output layer.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # We can implement embeddings as a simple lookup-table for given word \n",
    "        # indices\n",
    "        self.embedding = nn.Embedding(tokenizer.get_vocab_size(), 300)\n",
    "\n",
    "        # One-dimensional convolution-layer with 300 input channels, and 100  \n",
    "        # output channels as well as kernel size of 3; note that the\n",
    "        # one-dimensional convolutional layer has 3 dimensions\n",
    "        self.conv_1 = nn.Conv1d(300, 100, 3, padding=\"same\")\n",
    "\n",
    "        # Pooling with with a one-dimensional sliding window of length 3, \n",
    "        # reducing in this fashion the sequence length \n",
    "        self.pool_1 = nn.MaxPool1d(3)\n",
    "\n",
    "        # The input will be the reduced number of maximum picks from the\n",
    "        # previous operation; the dimension of those picks is the same as the\n",
    "        # output channel size from self.conv_1. We apply a different filter of \n",
    "        # size 5.\n",
    "        self.conv_2 = nn.Conv1d(100, 50, 5, padding=\"same\")\n",
    "\n",
    "        # Pooling with window size of 5\n",
    "        self.pool_2 = nn.MaxPool1d(5)\n",
    "\n",
    "        # Final fully connected linear layer from the 50 output channels to the\n",
    "        # 6 sentiment categories \n",
    "        self.linear_layer = nn.Linear(50, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Defining the forward pass of an input batch x.\n",
    "\n",
    "        Args:\n",
    "            x, tensor: The input is a batch of tweets from the data.\n",
    "        Returns:\n",
    "            y, float: The output are the logits from the final layer.\n",
    "        \"\"\"\n",
    "        # x will correspond here to a batch; therefore, the input dimensions of \n",
    "        # the embedding will be by PyTorch convention as follows:\n",
    "        # [batch_size, seq_len, emb_dim]\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Unfortunately the embedding tensor does not correspond to the shape \n",
    "        # that is needed for nn.Conv1d(); for this reason, we must switch its \n",
    "        # order to [batch_size, emb_dim, seq_len] for PyTorch\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # We can wrap the ReLu activation function around our convolution layer\n",
    "        # The output tensor will have the following shape: \n",
    "        # [batch_size, 100, seq_len]\n",
    "        x = nn.functional.relu(self.conv_1(x))\n",
    "\n",
    "        # Applying max pooling of size 3 means that the output length of the \n",
    "        # sequence is shrunk to seq_len//3\n",
    "        x = self.pool_1(x)\n",
    "\n",
    "        # Output of the following layer: [batch_size, 50, seq_len//3]\n",
    "        x = nn.functional.relu(self.conv_2(x))\n",
    "\n",
    "        # Shrinking the sequence length by 5\n",
    "        x = self.pool_2(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # At this point we have a tensor with 3 dimensions; however, the final layer \n",
    "        # requires an input of size [batch_size x 50]. To get this value we can \n",
    "        # aggregate the values and continue only with their mean\n",
    "        x = x.mean(dim=-1)\n",
    "\n",
    "        # In this fasion, the linear layer can be used to make predictions\n",
    "        y = self.linear_layer(x)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def fit(self, train_instances, val_instances, epochs, batch_size):\n",
    "        \"\"\" \n",
    "        Gradient based fitting method with Adam optimization and automatic \n",
    "        evaluation (F1 score) for each epoch.\n",
    "\n",
    "        Args:\n",
    "            train_instances, list: List of instance tuples.\n",
    "            val_instances, list: List of instance tuples.\n",
    "            epochs, int: Number of training epochs.\n",
    "            batch_size, int: Number of batch size.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.train()\n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "\n",
    "        self.val_score = []\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_batches = batching(train_instances, batch_size=batch_size, shuffle=True)\n",
    "            epoch_train_loss = 0.0\n",
    "            total_train_samples = 0\n",
    "\n",
    "            for inputs, labels in tqdm(train_batches):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = nn.functional.cross_entropy(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_train_loss += loss.item() * inputs.size(0)\n",
    "                total_train_samples += inputs.size(0)\n",
    "\n",
    "            avg_train_loss = epoch_train_loss / total_train_samples\n",
    "            self.train_losses.append(avg_train_loss)\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            self.eval()\n",
    "            val_batches = batching(val_instances, batch_size=batch_size, shuffle=False)\n",
    "            epoch_val_loss = 0.0\n",
    "            total_val_samples = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_batches:\n",
    "                    outputs = self(inputs)\n",
    "                    loss = nn.functional.cross_entropy(outputs, labels)\n",
    "                    epoch_val_loss += loss.item() * inputs.size(0)\n",
    "                    total_val_samples += inputs.size(0)\n",
    "\n",
    "            avg_val_loss = epoch_val_loss / total_val_samples\n",
    "            self.val_losses.append(avg_val_loss)\n",
    "\n",
    "            train_f1 = self.evaluate2(train_instances, batch_size=batch_size)\n",
    "            val_f1 = self.evaluate2(val_instances, batch_size=batch_size)\n",
    "            self.val_score.append(val_f1)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | \"\n",
    "                  f\"Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    def predict(self, input):\n",
    "        \"\"\" \n",
    "        To make inferences from the model.\n",
    "\n",
    "        Args:\n",
    "            input, tensor: Single instance.\n",
    "        Returns:\n",
    "            int: Integer for most probable class.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        outputs = self(input)\n",
    "\n",
    "        return torch.argmax(outputs, dim=-1)\n",
    "\n",
    "    def evaluate(self, instances, batch_size):\n",
    "        \"\"\" \n",
    "        To evaluate model's performance by various processes/standard.\n",
    "\n",
    "        Args:\n",
    "            instances, list: List of instance tuples.\n",
    "            batch_size, int: Batch size.\n",
    "        Returns:\n",
    "            float: Macro F1 score for given instances.\n",
    "        \"\"\"\n",
    "        batches = batching(instances, batch_size=batch_size, shuffle=False)\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "\n",
    "        for inputs, labels in batches:\n",
    "            y_test.extend(labels)\n",
    "            y_pred.extend(self.predict(inputs))\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='macro')\n",
    "        recall = recall_score(y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        print(\"CNN Classifier:\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1 Score: {f1}\")\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "        print(f\"Classification Report:\\n{report}\")\n",
    "    \n",
    "    def evaluate2(self, instances, batch_size):\n",
    "        \"\"\" \n",
    "        To make evaluations against the gold standard (true labels) from the \n",
    "        data.\n",
    "\n",
    "        Args:\n",
    "            instances, list: List of instance tuples.\n",
    "            batch_size, int: Batch size.\n",
    "        Returns:\n",
    "            float: Macro F1 score for given instances.\n",
    "        \"\"\"\n",
    "        batches = batching(instances, batch_size=batch_size, shuffle=False)\n",
    "        true = []\n",
    "        pred = []\n",
    "\n",
    "        for inputs, labels in batches:\n",
    "            true.extend(labels)\n",
    "            pred.extend(self.predict(inputs))\n",
    "\n",
    "        return f1_score(true, pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf02f634-df1b-498f-a9e2-b46b89ba98a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 246.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.9963 | Val Loss: 0.4386 | Train F1: 0.8452 | Val F1: 0.7876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 244.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 0.2718 | Val Loss: 0.3351 | Train F1: 0.9378 | Val F1: 0.8482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 237.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 0.1193 | Val Loss: 0.3373 | Train F1: 0.9739 | Val F1: 0.8456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 238.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 0.0704 | Val Loss: 0.3813 | Train F1: 0.9781 | Val F1: 0.8393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 237.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 0.0534 | Val Loss: 0.4793 | Train F1: 0.9799 | Val F1: 0.8356\n"
     ]
    }
   ],
   "source": [
    "classifier = CNN_Classifier()\n",
    "classifier.fit(train_instances, val_instances, epochs=5, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3b6ec90-a70f-4fb7-b02c-1261f9d563c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Classifier:\n",
      "Accuracy: 0.8675\n",
      "Precision: 0.8095429737045318\n",
      "Recall: 0.8213258506836061\n",
      "F1 Score: 0.8136135768933418\n",
      "Confusion Matrix:\n",
      "[[499  19   4  36  22   1]\n",
      " [  3 631  37  11   7   6]\n",
      " [  3  33 112   7   4   0]\n",
      " [  9   5   2 246  11   2]\n",
      " [  5   1   0   3 204  11]\n",
      " [  1   6   0   0  16  43]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91       581\n",
      "           1       0.91      0.91      0.91       695\n",
      "           2       0.72      0.70      0.71       159\n",
      "           3       0.81      0.89      0.85       275\n",
      "           4       0.77      0.91      0.84       224\n",
      "           5       0.68      0.65      0.67        66\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.81      0.82      0.81      2000\n",
      "weighted avg       0.87      0.87      0.87      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1_test = classifier.evaluate(test_instances, batch_size=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
