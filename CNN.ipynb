{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c1587f",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN) Model\n",
    "In this notebook, we implemented a CNN model for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d018716b-0cb0-43f3-abfd-ace6f613534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from tokenizers import Tokenizer, models, trainers\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2bc1d1",
   "metadata": {},
   "source": [
    "# Data Download\n",
    "Download the dataset (https://huggingface.co/datasets/dair-ai/emotion) from Hugging Face. Since the data has been separated into train, validation and test data by the owner, we can store them separately for further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e5d4d27-6fcc-4e38-ad99-5aedad8842fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = load_dataset(\"dair-ai/emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0315fef-7e42-4278-8dd5-6078ec8b754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c66816cb-e29d-4940-95bc-b785747447e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = emotions[\"train\"]\n",
    "validation_data = emotions[\"validation\"]\n",
    "test_data = emotions[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f3be92",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Train BPE tokenizer from Hugging Face library with our train data for use of tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25b2a4d5-ea1c-4196-844c-fddd2a6f2259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "vocab_n = 5000\n",
    "sequence_len = 64\n",
    "\n",
    "# Initialize a tokenizer using BPE (Byte Pair Encoding)\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.enable_padding(length=sequence_len)\n",
    "tokenizer.enable_truncation(max_length=sequence_len)\n",
    "tokenizer_trainer = trainers.BpeTrainer(vocab_size=vocab_n)\n",
    "tokenizer.train_from_iterator(train_data[\"text\"], trainer=tokenizer_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0a49c7",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "Define functions for pre-processing data into the form that our CNN model can read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8b3d861-7e4f-48d3-b643-06e9b9a88405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, tokenizer: Tokenizer):\n",
    "    \"\"\" \n",
    "    Helper function to tokenize text and return corresponding token IDs as tensors.\n",
    "\n",
    "    Args:\n",
    "        text, str: Text instance from training data.\n",
    "        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.\n",
    "    Returns:\n",
    "        Tensor: One-dimensional PyTorch tensor with token IDs.\n",
    "    \"\"\"\n",
    "    return torch.tensor(tokenizer.encode(text).ids)\n",
    "\n",
    "\n",
    "def preprocess_label(label: int):\n",
    "    \"\"\" \n",
    "    Helper function to return label as tensor.\n",
    "\n",
    "    Args:\n",
    "        label, int: Label from instance.\n",
    "    Returns:\n",
    "        Tensor: One-dimensional PyTorch tensor containing the label index.\n",
    "    \"\"\"\n",
    "    return torch.tensor(label)\n",
    "\n",
    "\n",
    "def preprocess(data: dict, tokenizer: Tokenizer):\n",
    "    \"\"\" \n",
    "    Transforms input dataset to tokenized vector representations.\n",
    "\n",
    "    Args:\n",
    "        data, dict: Dictionary with text instances and labels.\n",
    "        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.\n",
    "    Returns:\n",
    "        list: List with tensors for the input texts and labels.\n",
    "    \"\"\"\n",
    "    instances = []\n",
    "\n",
    "    for text, label in zip(data[\"text\"], data[\"label\"]):\n",
    "        input = preprocess_text(text, tokenizer)\n",
    "        label = preprocess_label(label)\n",
    "        \n",
    "        instances.append((input, label))\n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85779c8a-cb93-472c-8e61-8e7aadf46fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instances = preprocess(train_data, tokenizer)\n",
    "val_instances = preprocess(validation_data, tokenizer)\n",
    "test_instances = preprocess(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b311a26",
   "metadata": {},
   "source": [
    "# Batching\n",
    "Define a batching method that split training data into batches randomly for training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba01debf-56ad-4302-aa85-8b506e686a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching\n",
    "\n",
    "def batching(instances: list, batch_size: int, shuffle: bool):\n",
    "    \"\"\" \n",
    "    Batches input instances along the given size and returns list of batches.\n",
    "\n",
    "    Args:\n",
    "        instances, list: List of instances, containing a tuple of two tensors \n",
    "            for each text as well as corresponding label.\n",
    "        batch_size, int: Size for batches.\n",
    "        shuffle, bool: If true, the instances will be shuffled before batching.\n",
    "    Returns:\n",
    "        list: List containing tuples that correspond to single batches.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        random.shuffle(instances)\n",
    "\n",
    "    batches = []\n",
    "\n",
    "    # We iterate through the instances with batch_size steps\n",
    "    for i in range(0, len(instances), batch_size):\n",
    "\n",
    "        # Stacking the instances with dim=0 (default value)\n",
    "        batch_texts = torch.stack(\n",
    "            [instance[0] for instance in instances[i : i + batch_size]]\n",
    "        )\n",
    "        batch_labels = torch.stack(\n",
    "            [instance[1] for instance in instances[i : i + batch_size]]\n",
    "        )\n",
    "\n",
    "        batches.append((batch_texts, batch_labels))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c80a91",
   "metadata": {},
   "source": [
    "# Model Construction\n",
    "Construct the CNN model with torch library. For this project, we use two convolutional layers, each followed by max-pooling layer for reducing dimensions, and use one fully-connected layer for outputing the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5aa7e1e-9735-4c8d-8761-68d251b2036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Network\n",
    "\n",
    "class CNN_Classifier(nn.Module):\n",
    "    \"\"\" \n",
    "    CNN for sentiment classification with 6 classes, consisting of an embedding \n",
    "    layer, two convolutional layers with different filter sizes, different \n",
    "    pooling sizes, as well as one linear output layer.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # We can implement embeddings as a simple lookup-table for given word \n",
    "        # indices\n",
    "        self.embedding = nn.Embedding(tokenizer.get_vocab_size(), 300)\n",
    "\n",
    "        # One-dimensional convolution-layer with 300 input channels, and 100  \n",
    "        # output channels as well as kernel size of 3; note that the\n",
    "        # one-dimensional convolutional layer has 3 dimensions\n",
    "        self.conv_1 = nn.Conv1d(300, 100, 3, padding=\"same\")\n",
    "\n",
    "        # Pooling with with a one-dimensional sliding window of length 3, \n",
    "        # reducing in this fashion the sequence length \n",
    "        self.pool_1 = nn.MaxPool1d(3)\n",
    "\n",
    "        # The input will be the reduced number of maximum picks from the\n",
    "        # previous operation; the dimension of those picks is the same as the\n",
    "        # output channel size from self.conv_1. We apply a different filter of \n",
    "        # size 5.\n",
    "        self.conv_2 = nn.Conv1d(100, 50, 5, padding=\"same\")\n",
    "\n",
    "        # Pooling with window size of 5\n",
    "        self.pool_2 = nn.MaxPool1d(5)\n",
    "\n",
    "        # Final fully connected linear layer from the 50 output channels to the\n",
    "        # 6 sentiment categories \n",
    "        self.linear_layer = nn.Linear(50, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Defining the forward pass of an input batch x.\n",
    "\n",
    "        Args:\n",
    "            x, tensor: The input is a batch of tweets from the data.\n",
    "        Returns:\n",
    "            y, float: The output are the logits from the final layer.\n",
    "        \"\"\"\n",
    "        # x will correspond here to a batch; therefore, the input dimensions of \n",
    "        # the embedding will be by PyTorch convention as follows:\n",
    "        # [batch_size, seq_len, emb_dim]\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Unfortunately the embedding tensor does not correspond to the shape \n",
    "        # that is needed for nn.Conv1d(); for this reason, we must switch its \n",
    "        # order to [batch_size, emb_dim, seq_len] for PyTorch\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # We can wrap the ReLu activation function around our convolution layer\n",
    "        # The output tensor will have the following shape: \n",
    "        # [batch_size, 100, seq_len]\n",
    "        x = nn.functional.relu(self.conv_1(x))\n",
    "\n",
    "        # Applying max pooling of size 3 means that the output length of the \n",
    "        # sequence is shrunk to seq_len//3\n",
    "        x = self.pool_1(x)\n",
    "\n",
    "        # Output of the following layer: [batch_size, 50, seq_len//3]\n",
    "        x = nn.functional.relu(self.conv_2(x))\n",
    "\n",
    "        # Shrinking the sequence length by 5\n",
    "        x = self.pool_2(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # At this point we have a tensor with 3 dimensions; however, the final layer \n",
    "        # requires an input of size [batch_size x 50]. To get this value we can \n",
    "        # aggregate the values and continue only with their mean\n",
    "        x = x.mean(dim=-1)\n",
    "\n",
    "        # In this fasion, the linear layer can be used to make predictions\n",
    "        y = self.linear_layer(x)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def fit(self, train_instances, val_instances, epochs, batch_size):\n",
    "        \"\"\" \n",
    "        Gradient based fitting method with Adam optimization and automatic \n",
    "        evaluation (F1 score) for each epoch.\n",
    "\n",
    "        Args:\n",
    "            train_instances, list: List of instance tuples.\n",
    "            val_instances, list: List of instance tuples.\n",
    "            epochs, int: Number of training epochs.\n",
    "            batch_size, int: Number of batch size.\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        losses = []\n",
    "        val_loss = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_batches = batching(\n",
    "                train_instances,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True)\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for inputs, labels in tqdm(train_batches):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = nn.functional.cross_entropy(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            epoch_loss = running_loss / len(train_batches)\n",
    "            losses.append(epoch_loss)\n",
    "            train_f1 = self.evaluate2(train_instances, batch_size=batch_size)\n",
    "            val_f1 = self.evaluate2(val_instances, batch_size=batch_size)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1} train F1 score: {train_f1}, validation F1 score: {val_f1}\")\n",
    "        return losses\n",
    "\n",
    "    def predict(self, input):\n",
    "        \"\"\" \n",
    "        To make inferences from the model.\n",
    "\n",
    "        Args:\n",
    "            input, tensor: Single instance.\n",
    "        Returns:\n",
    "            int: Integer for most probable class.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        outputs = self(input)\n",
    "\n",
    "        return torch.argmax(outputs, dim=-1)\n",
    "\n",
    "    def evaluate(self, instances, batch_size):\n",
    "        \"\"\" \n",
    "        To evaluate model's performance by various processes/standard.\n",
    "\n",
    "        Args:\n",
    "            instances, list: List of instance tuples.\n",
    "            batch_size, int: Batch size.\n",
    "        Returns:\n",
    "            float: Macro F1 score for given instances.\n",
    "        \"\"\"\n",
    "        batches = batching(instances, batch_size=batch_size, shuffle=False)\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "\n",
    "        for inputs, labels in batches:\n",
    "            y_test.extend(labels)\n",
    "            y_pred.extend(self.predict(inputs))\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='macro')\n",
    "        recall = recall_score(y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        print(\"CNN Classifier:\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1 Score: {f1}\")\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "        print(f\"Classification Report:\\n{report}\")\n",
    "    \n",
    "    def evaluate2(self, instances, batch_size):\n",
    "        \"\"\" \n",
    "        To make evaluations against the gold standard (true labels) from the \n",
    "        data.\n",
    "\n",
    "        Args:\n",
    "            instances, list: List of instance tuples.\n",
    "            batch_size, int: Batch size.\n",
    "        Returns:\n",
    "            float: Macro F1 score for given instances.\n",
    "        \"\"\"\n",
    "        batches = batching(instances, batch_size=batch_size, shuffle=False)\n",
    "        true = []\n",
    "        pred = []\n",
    "\n",
    "        for inputs, labels in batches:\n",
    "            true.extend(labels)\n",
    "            pred.extend(self.predict(inputs))\n",
    "\n",
    "        return f1_score(true, pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ec2fe",
   "metadata": {},
   "source": [
    "# Training Process\n",
    "Train our CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf02f634-df1b-498f-a9e2-b46b89ba98a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 212.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train F1 score: 0.515517437343546, validation F1 score: 0.4853621675399859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 216.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train F1 score: 0.8758444915496089, validation F1 score: 0.7822224380777771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 213.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train F1 score: 0.9435702251912127, validation F1 score: 0.8487430247698464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 215.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train F1 score: 0.97428585874388, validation F1 score: 0.8657236060181356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 221.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train F1 score: 0.986418639012912, validation F1 score: 0.8728551825815218\n"
     ]
    }
   ],
   "source": [
    "classifier = CNN_Classifier()\n",
    "loss = classifier.fit(train_instances, val_instances, epochs=5, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31749c72",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "Evaluate the data using accuracy, recall, F1 score and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3b6ec90-a70f-4fb7-b02c-1261f9d563c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Classifier:\n",
      "Accuracy: 0.891\n",
      "Precision: 0.8408587854687636\n",
      "Recall: 0.8532607101805086\n",
      "F1 Score: 0.8458309710694181\n",
      "Confusion Matrix:\n",
      "[[528  15   9  21   8   0]\n",
      " [  1 641  47   4   1   1]\n",
      " [  1  22 132   4   0   0]\n",
      " [ 14   5   2 244   9   1]\n",
      " [  8   2   0  10 189  15]\n",
      " [  1   6   1   0  10  48]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93       581\n",
      "           1       0.93      0.92      0.92       695\n",
      "           2       0.69      0.83      0.75       159\n",
      "           3       0.86      0.89      0.87       275\n",
      "           4       0.87      0.84      0.86       224\n",
      "           5       0.74      0.73      0.73        66\n",
      "\n",
      "    accuracy                           0.89      2000\n",
      "   macro avg       0.84      0.85      0.85      2000\n",
      "weighted avg       0.90      0.89      0.89      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1_test = classifier.evaluate(test_instances, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5339a683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
