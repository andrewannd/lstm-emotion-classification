{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d47488aa-1a4c-4bb5-9d07-2534582d53e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from tokenizers import Tokenizer, models, trainers\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "dataset = load_dataset(\"dair-ai/emotion\", \"split\")\n",
    "labels = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "train_data = dataset[\"train\"]\n",
    "validation_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]\n",
    "# Tokenization\n",
    "vocab_n = 5000\n",
    "sequence_len = 64\n",
    "\n",
    "# Initialize a tokenizer using BPE (Byte Pair Encoding)\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.enable_padding(length=sequence_len)\n",
    "tokenizer.enable_truncation(max_length=sequence_len)\n",
    "tokenizer_trainer = trainers.BpeTrainer(vocab_size=vocab_n)\n",
    "tokenizer.train_from_iterator(train_data[\"text\"], trainer=tokenizer_trainer)\n",
    "def preprocess_text(text: str, tokenizer: Tokenizer):\n",
    "    \"\"\" \n",
    "    Helper function to tokenize text and return corresponding token IDs as tensors.\n",
    "\n",
    "    Args:\n",
    "        text, str: Text instance from training data.\n",
    "        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.\n",
    "    Returns:\n",
    "        Tensor: One-dimensional PyTorch tensor with token IDs.\n",
    "    \"\"\"\n",
    "    return torch.tensor(tokenizer.encode(text).ids)\n",
    "\n",
    "\n",
    "def preprocess_label(label: int):\n",
    "    \"\"\" \n",
    "    Helper function to return label as tensor.\n",
    "\n",
    "    Args:\n",
    "        label, int: Label from instance.\n",
    "    Returns:\n",
    "        Tensor: One-dimensional PyTorch tensor containing the label index.\n",
    "    \"\"\"\n",
    "    return torch.tensor(label)\n",
    "\n",
    "\n",
    "def preprocess(data: dict, tokenizer: Tokenizer):\n",
    "    \"\"\" \n",
    "    Transforms input dataset to tokenized vector representations.\n",
    "\n",
    "    Args:\n",
    "        data, dict: Dictionary with text instances and labels.\n",
    "        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.\n",
    "    Returns:\n",
    "        list: List with tensors for the input texts and labels.\n",
    "    \"\"\"\n",
    "    instances = []\n",
    "\n",
    "    for text, label in zip(data[\"text\"], data[\"label\"]):\n",
    "        input = preprocess_text(text, tokenizer)\n",
    "        label = preprocess_label(label)\n",
    "        \n",
    "        instances.append((input, label))\n",
    "\n",
    "    return instances\n",
    "train_instances = preprocess(train_data, tokenizer)\n",
    "val_instances = preprocess(validation_data, tokenizer)\n",
    "test_instances = preprocess(test_data, tokenizer)\n",
    "# Batching for LSTM input\n",
    "\n",
    "def batching_lstm(instances: list, batch_size: int, shuffle: bool):\n",
    "    \"\"\"\n",
    "    Batching for LSTM input: with padding support.\n",
    "\n",
    "    Args:\n",
    "        instances: List of (input_tensor, label_tensor) pairs.\n",
    "        batch_size: Number of instances per batch.\n",
    "        shuffle: Whether to shuffle the dataset before batching.\n",
    "    \n",
    "    Returns:\n",
    "        batches: List of (padded_input_tensor, label_tensor) for each batch.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        random.shuffle(instances)\n",
    "\n",
    "    batches = []\n",
    "\n",
    "    for i in range(0, len(instances), batch_size):\n",
    "        batch = instances[i : i + batch_size]\n",
    "\n",
    "        # Take out a batch of inputs and labels\n",
    "        batch_inputs = [item[0] for item in batch]  # list of tensors (seq_len,)\n",
    "        batch_labels = torch.stack([item[1] for item in batch])  # tensor of shape [batch_size]\n",
    "\n",
    "        # Automatic padding becomes [batch_size, max_seq_len]\n",
    "        padded_inputs = pad_sequence(batch_inputs, batch_first=True, padding_value=0)\n",
    "\n",
    "        batches.append((padded_inputs, batch_labels))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a6701be-e017-4873-883f-6fc0b062fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, padding_idx):\n",
    "        super(CNN_Classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=100, out_channels=100, kernel_size=3, padding=1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # [batch, seq_len, emb_dim]\n",
    "        embedded = embedded.transpose(1, 2)  # [batch, emb_dim, seq_len]\n",
    "        \n",
    "        conv1_out = nn.functional.relu(self.conv1(embedded))  # [batch, 100, seq_len]\n",
    "        conv2_out = nn.functional.relu(self.conv2(conv1_out))  # [batch, 100, seq_len]\n",
    "        \n",
    "        return self.dropout(conv2_out.transpose(1, 2))  # [batch, seq_len, 100]\n",
    "\n",
    "class LSTM_Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, padding_idx=None): \n",
    "        super(LSTM_Classifier, self).__init__()\n",
    "\n",
    "        # Single-layer bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Fully connected layer, output 6 types of emotions\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, cnn_features):\n",
    "         # cnn_features: [batch, seq_len, 100]\n",
    "        lstm_out, (hidden, _) = self.lstm(cnn_features)\n",
    "\n",
    "        # Take the last hidden layer of the forward and reverse directions and concatenate them\n",
    "        hidden_forward = hidden[-2, :, :]  # [batch, hidden_dim]\n",
    "        hidden_backward = hidden[-1, :, :]  # [batch, hidden_dim]\n",
    "        combined = torch.cat((hidden_forward, hidden_backward), dim=1)  # [batch, hidden_dim * 2]\n",
    "\n",
    "        return self.fc(self.dropout(combined))  # [batch, output_dim]\n",
    "    \n",
    "# Get the vocabulary dictionary from the tokenizer (word â†’ ID)\n",
    "word2idx = tokenizer.get_vocab()  # e.g., {'i': 4, 'love': 5, 'this': 6, ...}\n",
    "\n",
    "# Reversal\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "padding_idx = word2idx.get(\"[PAD]\", 0) \n",
    "\n",
    "class CNN_LSTM_Ensemble(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, padding_idx):\n",
    "        super(CNN_LSTM_Ensemble, self).__init__()\n",
    "        self.cnn = CNN_Classifier(vocab_size=vocab_size, embedding_dim=embedding_dim, output_dim=100, padding_idx=padding_idx)\n",
    "        self.lstm = LSTM_Classifier(input_dim=100, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "        self.fc = nn.Linear(output_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cnn_features = self.cnn(x)  # [batch, seq_len, 100]\n",
    "        return self.lstm(cnn_features)  # [batch, output_dim]\n",
    "        \n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def train_and_evaluate(model, train_batches, val_batches, num_epochs=25, lr=1e-3, device=\"gpu\"):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for batch_x, batch_y in train_batches:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)  # shape: [batch_size, 6]\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "        train_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "        # ---- verify ----\n",
    "        model.eval()\n",
    "        val_preds, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for val_x, val_y in val_batches:\n",
    "                val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "                val_out = model(val_x)\n",
    "                val_pred = torch.argmax(val_out, dim=1)\n",
    "                val_preds.extend(val_pred.cpu().numpy())\n",
    "                val_labels.extend(val_y.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {sum(train_losses)/len(train_losses):.4f} | \"\n",
    "              f\"Train Acc: {train_acc:.4f} | F1: {train_f1:.4f} || \"\n",
    "              f\"Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cc2113a-12f0-412f-8c0a-bccd4f2f2b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 1.5645 | Train Acc: 0.3524 | F1: 0.2611 || Val Acc: 0.4280 | Val F1: 0.3452\n",
      "Epoch 2/100 | Train Loss: 1.1336 | Train Acc: 0.5672 | F1: 0.5167 || Val Acc: 0.6560 | Val F1: 0.6091\n",
      "Epoch 3/100 | Train Loss: 0.7258 | Train Acc: 0.7255 | F1: 0.6971 || Val Acc: 0.7590 | Val F1: 0.7384\n",
      "Epoch 4/100 | Train Loss: 0.4815 | Train Acc: 0.8305 | F1: 0.8158 || Val Acc: 0.8300 | Val F1: 0.8154\n",
      "Epoch 5/100 | Train Loss: 0.3605 | Train Acc: 0.8734 | F1: 0.8612 || Val Acc: 0.8355 | Val F1: 0.8189\n",
      "Epoch 6/100 | Train Loss: 0.2810 | Train Acc: 0.8981 | F1: 0.8906 || Val Acc: 0.8605 | Val F1: 0.8595\n",
      "Epoch 7/100 | Train Loss: 0.2221 | Train Acc: 0.9229 | F1: 0.9224 || Val Acc: 0.8600 | Val F1: 0.8620\n",
      "Epoch 8/100 | Train Loss: 0.1849 | Train Acc: 0.9358 | F1: 0.9357 || Val Acc: 0.8770 | Val F1: 0.8770\n",
      "Epoch 9/100 | Train Loss: 0.1433 | Train Acc: 0.9507 | F1: 0.9507 || Val Acc: 0.8665 | Val F1: 0.8665\n",
      "Epoch 10/100 | Train Loss: 0.1294 | Train Acc: 0.9531 | F1: 0.9530 || Val Acc: 0.8780 | Val F1: 0.8772\n",
      "Epoch 11/100 | Train Loss: 0.1070 | Train Acc: 0.9622 | F1: 0.9621 || Val Acc: 0.8825 | Val F1: 0.8829\n",
      "Epoch 12/100 | Train Loss: 0.0919 | Train Acc: 0.9672 | F1: 0.9671 || Val Acc: 0.8855 | Val F1: 0.8852\n",
      "Epoch 13/100 | Train Loss: 0.0901 | Train Acc: 0.9677 | F1: 0.9677 || Val Acc: 0.8900 | Val F1: 0.8882\n",
      "Epoch 14/100 | Train Loss: 0.0814 | Train Acc: 0.9701 | F1: 0.9701 || Val Acc: 0.8915 | Val F1: 0.8919\n",
      "Epoch 15/100 | Train Loss: 0.0730 | Train Acc: 0.9742 | F1: 0.9742 || Val Acc: 0.8865 | Val F1: 0.8865\n",
      "Epoch 16/100 | Train Loss: 0.0639 | Train Acc: 0.9787 | F1: 0.9787 || Val Acc: 0.8900 | Val F1: 0.8893\n",
      "Epoch 17/100 | Train Loss: 0.0530 | Train Acc: 0.9813 | F1: 0.9813 || Val Acc: 0.8950 | Val F1: 0.8957\n",
      "Epoch 18/100 | Train Loss: 0.0488 | Train Acc: 0.9819 | F1: 0.9819 || Val Acc: 0.8960 | Val F1: 0.8966\n",
      "Epoch 19/100 | Train Loss: 0.0502 | Train Acc: 0.9816 | F1: 0.9816 || Val Acc: 0.8875 | Val F1: 0.8874\n",
      "Epoch 20/100 | Train Loss: 0.0438 | Train Acc: 0.9850 | F1: 0.9850 || Val Acc: 0.8895 | Val F1: 0.8905\n",
      "Epoch 21/100 | Train Loss: 0.0448 | Train Acc: 0.9836 | F1: 0.9836 || Val Acc: 0.8990 | Val F1: 0.9002\n",
      "Epoch 22/100 | Train Loss: 0.0414 | Train Acc: 0.9858 | F1: 0.9858 || Val Acc: 0.8855 | Val F1: 0.8840\n",
      "Epoch 23/100 | Train Loss: 0.0425 | Train Acc: 0.9857 | F1: 0.9857 || Val Acc: 0.8990 | Val F1: 0.9002\n",
      "Epoch 24/100 | Train Loss: 0.0434 | Train Acc: 0.9855 | F1: 0.9855 || Val Acc: 0.8960 | Val F1: 0.8967\n",
      "Epoch 25/100 | Train Loss: 0.0399 | Train Acc: 0.9862 | F1: 0.9863 || Val Acc: 0.8990 | Val F1: 0.8995\n",
      "Epoch 26/100 | Train Loss: 0.0315 | Train Acc: 0.9892 | F1: 0.9892 || Val Acc: 0.9000 | Val F1: 0.9010\n",
      "Epoch 27/100 | Train Loss: 0.0307 | Train Acc: 0.9886 | F1: 0.9886 || Val Acc: 0.8990 | Val F1: 0.8998\n",
      "Epoch 28/100 | Train Loss: 0.0295 | Train Acc: 0.9897 | F1: 0.9897 || Val Acc: 0.8955 | Val F1: 0.8962\n",
      "Epoch 29/100 | Train Loss: 0.0276 | Train Acc: 0.9902 | F1: 0.9903 || Val Acc: 0.8985 | Val F1: 0.9005\n",
      "Epoch 30/100 | Train Loss: 0.0313 | Train Acc: 0.9888 | F1: 0.9888 || Val Acc: 0.9000 | Val F1: 0.9008\n",
      "Epoch 31/100 | Train Loss: 0.0258 | Train Acc: 0.9917 | F1: 0.9917 || Val Acc: 0.8960 | Val F1: 0.8966\n",
      "Epoch 32/100 | Train Loss: 0.0331 | Train Acc: 0.9888 | F1: 0.9888 || Val Acc: 0.9020 | Val F1: 0.9010\n",
      "Epoch 33/100 | Train Loss: 0.0284 | Train Acc: 0.9906 | F1: 0.9906 || Val Acc: 0.8945 | Val F1: 0.8940\n",
      "Epoch 34/100 | Train Loss: 0.0218 | Train Acc: 0.9923 | F1: 0.9923 || Val Acc: 0.8990 | Val F1: 0.8998\n",
      "Epoch 35/100 | Train Loss: 0.0265 | Train Acc: 0.9913 | F1: 0.9913 || Val Acc: 0.9005 | Val F1: 0.9011\n",
      "Epoch 36/100 | Train Loss: 0.0217 | Train Acc: 0.9925 | F1: 0.9925 || Val Acc: 0.9040 | Val F1: 0.9037\n",
      "Epoch 37/100 | Train Loss: 0.0231 | Train Acc: 0.9917 | F1: 0.9917 || Val Acc: 0.9000 | Val F1: 0.9002\n",
      "Epoch 38/100 | Train Loss: 0.0222 | Train Acc: 0.9929 | F1: 0.9929 || Val Acc: 0.9035 | Val F1: 0.9040\n",
      "Epoch 39/100 | Train Loss: 0.0216 | Train Acc: 0.9927 | F1: 0.9927 || Val Acc: 0.9010 | Val F1: 0.9009\n",
      "Epoch 40/100 | Train Loss: 0.0209 | Train Acc: 0.9926 | F1: 0.9926 || Val Acc: 0.9015 | Val F1: 0.9011\n",
      "Epoch 41/100 | Train Loss: 0.0268 | Train Acc: 0.9909 | F1: 0.9909 || Val Acc: 0.9030 | Val F1: 0.9029\n",
      "Epoch 42/100 | Train Loss: 0.0202 | Train Acc: 0.9934 | F1: 0.9934 || Val Acc: 0.9000 | Val F1: 0.9009\n",
      "Epoch 43/100 | Train Loss: 0.0233 | Train Acc: 0.9918 | F1: 0.9918 || Val Acc: 0.9020 | Val F1: 0.9026\n",
      "Epoch 44/100 | Train Loss: 0.0183 | Train Acc: 0.9938 | F1: 0.9938 || Val Acc: 0.9070 | Val F1: 0.9077\n",
      "Epoch 45/100 | Train Loss: 0.0207 | Train Acc: 0.9924 | F1: 0.9924 || Val Acc: 0.9040 | Val F1: 0.9042\n",
      "Epoch 46/100 | Train Loss: 0.0145 | Train Acc: 0.9951 | F1: 0.9951 || Val Acc: 0.9065 | Val F1: 0.9072\n",
      "Epoch 47/100 | Train Loss: 0.0180 | Train Acc: 0.9939 | F1: 0.9939 || Val Acc: 0.9025 | Val F1: 0.9029\n",
      "Epoch 48/100 | Train Loss: 0.0203 | Train Acc: 0.9925 | F1: 0.9925 || Val Acc: 0.9010 | Val F1: 0.9012\n",
      "Epoch 49/100 | Train Loss: 0.0193 | Train Acc: 0.9935 | F1: 0.9935 || Val Acc: 0.9025 | Val F1: 0.9033\n",
      "Epoch 50/100 | Train Loss: 0.0179 | Train Acc: 0.9935 | F1: 0.9935 || Val Acc: 0.9030 | Val F1: 0.9026\n",
      "Epoch 51/100 | Train Loss: 0.0194 | Train Acc: 0.9929 | F1: 0.9929 || Val Acc: 0.8950 | Val F1: 0.8955\n",
      "Epoch 52/100 | Train Loss: 0.0175 | Train Acc: 0.9938 | F1: 0.9938 || Val Acc: 0.9045 | Val F1: 0.9049\n",
      "Epoch 53/100 | Train Loss: 0.0204 | Train Acc: 0.9929 | F1: 0.9929 || Val Acc: 0.9000 | Val F1: 0.9012\n",
      "Epoch 54/100 | Train Loss: 0.0228 | Train Acc: 0.9924 | F1: 0.9924 || Val Acc: 0.8995 | Val F1: 0.8998\n",
      "Epoch 55/100 | Train Loss: 0.0181 | Train Acc: 0.9939 | F1: 0.9939 || Val Acc: 0.9010 | Val F1: 0.9020\n",
      "Epoch 56/100 | Train Loss: 0.0224 | Train Acc: 0.9921 | F1: 0.9921 || Val Acc: 0.8920 | Val F1: 0.8920\n",
      "Epoch 57/100 | Train Loss: 0.0222 | Train Acc: 0.9931 | F1: 0.9931 || Val Acc: 0.9010 | Val F1: 0.9016\n",
      "Epoch 58/100 | Train Loss: 0.0156 | Train Acc: 0.9944 | F1: 0.9944 || Val Acc: 0.8970 | Val F1: 0.8975\n",
      "Epoch 59/100 | Train Loss: 0.0146 | Train Acc: 0.9946 | F1: 0.9946 || Val Acc: 0.8965 | Val F1: 0.8976\n",
      "Epoch 60/100 | Train Loss: 0.0138 | Train Acc: 0.9955 | F1: 0.9955 || Val Acc: 0.9020 | Val F1: 0.9025\n",
      "Epoch 61/100 | Train Loss: 0.0161 | Train Acc: 0.9940 | F1: 0.9940 || Val Acc: 0.8985 | Val F1: 0.8986\n",
      "Epoch 62/100 | Train Loss: 0.0163 | Train Acc: 0.9936 | F1: 0.9936 || Val Acc: 0.9025 | Val F1: 0.9025\n",
      "Epoch 63/100 | Train Loss: 0.0180 | Train Acc: 0.9932 | F1: 0.9932 || Val Acc: 0.9035 | Val F1: 0.9041\n",
      "Epoch 64/100 | Train Loss: 0.0153 | Train Acc: 0.9941 | F1: 0.9941 || Val Acc: 0.8995 | Val F1: 0.9006\n",
      "Epoch 65/100 | Train Loss: 0.0170 | Train Acc: 0.9938 | F1: 0.9938 || Val Acc: 0.9020 | Val F1: 0.9030\n",
      "Epoch 66/100 | Train Loss: 0.0119 | Train Acc: 0.9957 | F1: 0.9957 || Val Acc: 0.8980 | Val F1: 0.8985\n",
      "Epoch 67/100 | Train Loss: 0.0111 | Train Acc: 0.9959 | F1: 0.9959 || Val Acc: 0.8955 | Val F1: 0.8970\n",
      "Epoch 68/100 | Train Loss: 0.0149 | Train Acc: 0.9949 | F1: 0.9949 || Val Acc: 0.9040 | Val F1: 0.9049\n",
      "Epoch 69/100 | Train Loss: 0.0146 | Train Acc: 0.9946 | F1: 0.9946 || Val Acc: 0.9040 | Val F1: 0.9050\n",
      "Epoch 70/100 | Train Loss: 0.0125 | Train Acc: 0.9955 | F1: 0.9955 || Val Acc: 0.8995 | Val F1: 0.9000\n",
      "Epoch 71/100 | Train Loss: 0.0092 | Train Acc: 0.9968 | F1: 0.9968 || Val Acc: 0.9005 | Val F1: 0.9015\n",
      "Epoch 72/100 | Train Loss: 0.0218 | Train Acc: 0.9933 | F1: 0.9933 || Val Acc: 0.9010 | Val F1: 0.9016\n",
      "Epoch 73/100 | Train Loss: 0.0153 | Train Acc: 0.9948 | F1: 0.9948 || Val Acc: 0.9065 | Val F1: 0.9065\n",
      "Epoch 74/100 | Train Loss: 0.0146 | Train Acc: 0.9946 | F1: 0.9946 || Val Acc: 0.9065 | Val F1: 0.9071\n",
      "Epoch 75/100 | Train Loss: 0.0160 | Train Acc: 0.9946 | F1: 0.9946 || Val Acc: 0.9015 | Val F1: 0.9025\n",
      "Epoch 76/100 | Train Loss: 0.0166 | Train Acc: 0.9941 | F1: 0.9941 || Val Acc: 0.8995 | Val F1: 0.8999\n",
      "Epoch 77/100 | Train Loss: 0.0136 | Train Acc: 0.9949 | F1: 0.9949 || Val Acc: 0.9025 | Val F1: 0.9037\n",
      "Epoch 78/100 | Train Loss: 0.0130 | Train Acc: 0.9951 | F1: 0.9951 || Val Acc: 0.9045 | Val F1: 0.9045\n",
      "Epoch 79/100 | Train Loss: 0.0114 | Train Acc: 0.9959 | F1: 0.9959 || Val Acc: 0.9055 | Val F1: 0.9054\n",
      "Epoch 80/100 | Train Loss: 0.0118 | Train Acc: 0.9958 | F1: 0.9958 || Val Acc: 0.9030 | Val F1: 0.9027\n",
      "Epoch 81/100 | Train Loss: 0.0120 | Train Acc: 0.9954 | F1: 0.9954 || Val Acc: 0.9085 | Val F1: 0.9090\n",
      "Epoch 82/100 | Train Loss: 0.0134 | Train Acc: 0.9954 | F1: 0.9954 || Val Acc: 0.9095 | Val F1: 0.9099\n",
      "Epoch 83/100 | Train Loss: 0.0122 | Train Acc: 0.9951 | F1: 0.9951 || Val Acc: 0.9030 | Val F1: 0.9033\n",
      "Epoch 84/100 | Train Loss: 0.0101 | Train Acc: 0.9958 | F1: 0.9958 || Val Acc: 0.9060 | Val F1: 0.9061\n",
      "Epoch 85/100 | Train Loss: 0.0148 | Train Acc: 0.9940 | F1: 0.9940 || Val Acc: 0.9005 | Val F1: 0.9020\n",
      "Epoch 86/100 | Train Loss: 0.0138 | Train Acc: 0.9947 | F1: 0.9947 || Val Acc: 0.9140 | Val F1: 0.9144\n",
      "Epoch 87/100 | Train Loss: 0.0162 | Train Acc: 0.9946 | F1: 0.9946 || Val Acc: 0.9090 | Val F1: 0.9088\n",
      "Epoch 88/100 | Train Loss: 0.0118 | Train Acc: 0.9953 | F1: 0.9953 || Val Acc: 0.9145 | Val F1: 0.9149\n",
      "Epoch 89/100 | Train Loss: 0.0095 | Train Acc: 0.9964 | F1: 0.9964 || Val Acc: 0.9120 | Val F1: 0.9118\n",
      "Epoch 90/100 | Train Loss: 0.0111 | Train Acc: 0.9960 | F1: 0.9960 || Val Acc: 0.9115 | Val F1: 0.9115\n",
      "Epoch 91/100 | Train Loss: 0.0102 | Train Acc: 0.9966 | F1: 0.9966 || Val Acc: 0.9095 | Val F1: 0.9101\n",
      "Epoch 92/100 | Train Loss: 0.0107 | Train Acc: 0.9956 | F1: 0.9956 || Val Acc: 0.9110 | Val F1: 0.9111\n",
      "Epoch 93/100 | Train Loss: 0.0112 | Train Acc: 0.9957 | F1: 0.9957 || Val Acc: 0.9085 | Val F1: 0.9090\n",
      "Epoch 94/100 | Train Loss: 0.0136 | Train Acc: 0.9947 | F1: 0.9947 || Val Acc: 0.9040 | Val F1: 0.9046\n",
      "Epoch 95/100 | Train Loss: 0.0120 | Train Acc: 0.9957 | F1: 0.9957 || Val Acc: 0.9100 | Val F1: 0.9101\n",
      "Epoch 96/100 | Train Loss: 0.0110 | Train Acc: 0.9958 | F1: 0.9958 || Val Acc: 0.9100 | Val F1: 0.9101\n",
      "Epoch 97/100 | Train Loss: 0.0132 | Train Acc: 0.9947 | F1: 0.9947 || Val Acc: 0.9105 | Val F1: 0.9110\n",
      "Epoch 98/100 | Train Loss: 0.0126 | Train Acc: 0.9958 | F1: 0.9958 || Val Acc: 0.9065 | Val F1: 0.9065\n",
      "Epoch 99/100 | Train Loss: 0.0131 | Train Acc: 0.9952 | F1: 0.9952 || Val Acc: 0.9065 | Val F1: 0.9066\n",
      "Epoch 100/100 | Train Loss: 0.0118 | Train Acc: 0.9952 | F1: 0.9953 || Val Acc: 0.9090 | Val F1: 0.9090\n"
     ]
    }
   ],
   "source": [
    "# Parameters tuning\n",
    "model = CNN_LSTM_Ensemble(\n",
    "    vocab_size=len(word2idx),        # vocabulary size\n",
    "    embedding_dim=100,               # Dimensions of word vectors\n",
    "    hidden_dim=128,                  # Hidden layer dimensions\n",
    "    output_dim=6,                    # The number of output categories\n",
    "    padding_idx=word2idx.get(\"[PAD]\", 0)  # Index of pad token\n",
    ")\n",
    "train_instances = preprocess(dataset[\"train\"], tokenizer)\n",
    "val_instances = preprocess(dataset[\"validation\"], tokenizer)\n",
    "\n",
    "train_batches = batching_lstm(train_instances, batch_size=32, shuffle=True)\n",
    "val_batches = batching_lstm(val_instances, batch_size=32, shuffle=False)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_and_evaluate(model, train_batches, val_batches, num_epochs=100, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07e1abf2-6dcd-4a3d-8643-e21b5c664af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.94      0.94      0.94       550\n",
      "         joy       0.93      0.94      0.93       704\n",
      "        love       0.77      0.79      0.78       178\n",
      "       anger       0.90      0.92      0.91       275\n",
      "        fear       0.89      0.87      0.88       212\n",
      "    surprise       0.89      0.81      0.85        81\n",
      "\n",
      "    accuracy                           0.91      2000\n",
      "   macro avg       0.89      0.88      0.88      2000\n",
      "weighted avg       0.91      0.91      0.91      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get the label name order\n",
    "label_names = dataset[\"train\"].features[\"label\"].names  # ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "\n",
    "# Model prediction validation set\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in val_batches:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model(x_batch)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        true_labels.extend(y_batch.cpu().numpy())\n",
    "        pred_labels.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Print each category's precision, recall, f1-score\n",
    "print(classification_report(true_labels, pred_labels, target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54d764f3-a6e6-4f97-8813-dc7be4bebad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported: True\n",
      "CUDA version: 12.4\n",
      "Current device: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA is supported:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Current device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"æ— \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
