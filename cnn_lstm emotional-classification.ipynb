{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d47488aa-1a4c-4bb5-9d07-2534582d53e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from tokenizers import Tokenizer, models, trainers\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "dataset = load_dataset(\"dair-ai/emotion\", \"split\")\n",
    "labels = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "train_data = dataset[\"train\"]\n",
    "validation_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]\n",
    "# Tokenization\n",
    "vocab_n = 5000\n",
    "sequence_len = 64\n",
    "\n",
    "# Initialize a tokenizer using BPE (Byte Pair Encoding)\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.enable_padding(length=sequence_len)\n",
    "tokenizer.enable_truncation(max_length=sequence_len)\n",
    "tokenizer_trainer = trainers.BpeTrainer(vocab_size=vocab_n)\n",
    "tokenizer.train_from_iterator(train_data[\"text\"], trainer=tokenizer_trainer)\n",
    "def preprocess_text(text: str, tokenizer: Tokenizer):\n",
    "    \"\"\" \n",
    "    Helper function to tokenize text and return corresponding token IDs as tensors.\n",
    "\n",
    "    Args:\n",
    "        text, str: Text instance from training data.\n",
    "        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.\n",
    "    Returns:\n",
    "        Tensor: One-dimensional PyTorch tensor with token IDs.\n",
    "    \"\"\"\n",
    "    return torch.tensor(tokenizer.encode(text).ids)\n",
    "\n",
    "\n",
    "def preprocess_label(label: int):\n",
    "    \"\"\" \n",
    "    Helper function to return label as tensor.\n",
    "\n",
    "    Args:\n",
    "        label, int: Label from instance.\n",
    "    Returns:\n",
    "        Tensor: One-dimensional PyTorch tensor containing the label index.\n",
    "    \"\"\"\n",
    "    return torch.tensor(label)\n",
    "\n",
    "\n",
    "def preprocess(data: dict, tokenizer: Tokenizer):\n",
    "    \"\"\" \n",
    "    Transforms input dataset to tokenized vector representations.\n",
    "\n",
    "    Args:\n",
    "        data, dict: Dictionary with text instances and labels.\n",
    "        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.\n",
    "    Returns:\n",
    "        list: List with tensors for the input texts and labels.\n",
    "    \"\"\"\n",
    "    instances = []\n",
    "\n",
    "    for text, label in zip(data[\"text\"], data[\"label\"]):\n",
    "        input = preprocess_text(text, tokenizer)\n",
    "        label = preprocess_label(label)\n",
    "        \n",
    "        instances.append((input, label))\n",
    "\n",
    "    return instances\n",
    "train_instances = preprocess(train_data, tokenizer)\n",
    "val_instances = preprocess(validation_data, tokenizer)\n",
    "test_instances = preprocess(test_data, tokenizer)\n",
    "# Batching for LSTM input\n",
    "\n",
    "def batching_lstm(instances: list, batch_size: int, shuffle: bool):\n",
    "    \"\"\"\n",
    "    Batching for LSTM input: with padding support.\n",
    "\n",
    "    Args:\n",
    "        instances: List of (input_tensor, label_tensor) pairs.\n",
    "        batch_size: Number of instances per batch.\n",
    "        shuffle: Whether to shuffle the dataset before batching.\n",
    "    \n",
    "    Returns:\n",
    "        batches: List of (padded_input_tensor, label_tensor) for each batch.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        random.shuffle(instances)\n",
    "\n",
    "    batches = []\n",
    "\n",
    "    for i in range(0, len(instances), batch_size):\n",
    "        batch = instances[i : i + batch_size]\n",
    "\n",
    "        # Take out a batch of inputs and labels\n",
    "        batch_inputs = [item[0] for item in batch]  # list of tensors (seq_len,)\n",
    "        batch_labels = torch.stack([item[1] for item in batch])  # tensor of shape [batch_size]\n",
    "\n",
    "        # Automatic padding becomes [batch_size, max_seq_len]\n",
    "        padded_inputs = pad_sequence(batch_inputs, batch_first=True, padding_value=0)\n",
    "\n",
    "        batches.append((padded_inputs, batch_labels))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a6701be-e017-4873-883f-6fc0b062fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, padding_idx):\n",
    "        super(CNN_Classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=100, out_channels=100, kernel_size=3, padding=1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # [batch, seq_len, emb_dim]\n",
    "        embedded = embedded.transpose(1, 2)  # [batch, emb_dim, seq_len]\n",
    "        \n",
    "        conv1_out = F.relu(self.conv1(embedded))  # [batch, 100, seq_len]\n",
    "        conv2_out = F.relu(self.conv2(conv1_out))  # [batch, 100, seq_len]\n",
    "        \n",
    "        return self.dropout(conv2_out.transpose(1, 2))  # [batch, seq_len, 100]\n",
    "\n",
    "class LSTM_Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, padding_idx=None): \n",
    "        super(LSTM_Classifier, self).__init__()\n",
    "\n",
    "        # Single-layer bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            bidirectional=False,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Fully connected layer, output 6 types of emotions\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, cnn_features):\n",
    "         # cnn_features: [batch, seq_len, 100]\n",
    "        lstm_out, (hidden, _) = self.lstm(cnn_features)\n",
    "\n",
    "         # hidden: [num_layers, batch, hidden_dim]\n",
    "        last_hidden = hidden[-1]         # [batch, hidden_dim]\n",
    "        dropped = self.dropout(last_hidden)\n",
    "        return self.fc(dropped)          # [batch, output_dim]\n",
    "\n",
    "# Get the vocabulary dictionary from the tokenizer \n",
    "word2idx = tokenizer.get_vocab()  \n",
    "\n",
    "# Reversal\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "padding_idx = word2idx.get(\"[PAD]\", 0) \n",
    "\n",
    "class CNN_LSTM_Ensemble(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, padding_idx):\n",
    "        super(CNN_LSTM_Ensemble, self).__init__()\n",
    "        self.cnn = CNN_Classifier(vocab_size=vocab_size, embedding_dim=embedding_dim, output_dim=100, padding_idx=padding_idx)\n",
    "        # Map the CNN features to emotion categories after pooling\n",
    "        self.cnn_fc  = nn.Linear(100, output_dim)\n",
    "        self.lstm = LSTM_Classifier(input_dim=100, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "        # Fusion layer: Combine the two logits and then map them back to output_dim\n",
    "        self.fusion  = nn.Linear(output_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len]\n",
    "        \n",
    "        cnn_feats   = self.cnn(x)                       # [B, seq_len, 100]\n",
    "        cnn_repr    = torch.max(cnn_feats, dim=1)[0]    # [B, 100]  global max-pooling\n",
    "        cnn_logits  = self.cnn_fc(cnn_repr)             # [B, output_dim]\n",
    "\n",
    "        lstm_logits = self.lstm(cnn_feats)              # [B, output_dim]\n",
    "\n",
    "        combined    = torch.cat([cnn_logits, lstm_logits], dim=1)  # [B, output_dim*2]\n",
    "        return self.fusion(combined)                                # [B, output_dim]\n",
    "        \n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def train_and_evaluate(model, train_batches, val_batches, num_epochs=50, lr=1e-3, device=\"gpu\"):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for batch_x, batch_y in train_batches:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)  # shape: [batch_size, 6]\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "        train_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "        # verify\n",
    "        model.eval()\n",
    "        val_preds, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for val_x, val_y in val_batches:\n",
    "                val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "                val_out = model(val_x)\n",
    "                val_pred = torch.argmax(val_out, dim=1)\n",
    "                val_preds.extend(val_pred.cpu().numpy())\n",
    "                val_labels.extend(val_y.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {sum(train_losses)/len(train_losses):.4f} | \"\n",
    "              f\"Train Acc: {train_acc:.4f} | F1: {train_f1:.4f} || \"\n",
    "              f\"Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cc2113a-12f0-412f-8c0a-bccd4f2f2b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 1.4011 | Train Acc: 0.4482 | F1: 0.3835 || Val Acc: 0.6505 | Val F1: 0.5947\n",
      "Epoch 2/50 | Train Loss: 0.6615 | Train Acc: 0.7648 | F1: 0.7389 || Val Acc: 0.8035 | Val F1: 0.7986\n",
      "Epoch 3/50 | Train Loss: 0.3347 | Train Acc: 0.8840 | F1: 0.8810 || Val Acc: 0.8720 | Val F1: 0.8723\n",
      "Epoch 4/50 | Train Loss: 0.1983 | Train Acc: 0.9288 | F1: 0.9283 || Val Acc: 0.8860 | Val F1: 0.8866\n",
      "Epoch 5/50 | Train Loss: 0.1311 | Train Acc: 0.9525 | F1: 0.9523 || Val Acc: 0.8930 | Val F1: 0.8938\n",
      "Epoch 6/50 | Train Loss: 0.0933 | Train Acc: 0.9679 | F1: 0.9679 || Val Acc: 0.8925 | Val F1: 0.8934\n",
      "Epoch 7/50 | Train Loss: 0.0723 | Train Acc: 0.9742 | F1: 0.9742 || Val Acc: 0.8775 | Val F1: 0.8785\n",
      "Epoch 8/50 | Train Loss: 0.0627 | Train Acc: 0.9783 | F1: 0.9783 || Val Acc: 0.8850 | Val F1: 0.8862\n",
      "Epoch 9/50 | Train Loss: 0.0591 | Train Acc: 0.9792 | F1: 0.9792 || Val Acc: 0.8865 | Val F1: 0.8877\n",
      "Epoch 10/50 | Train Loss: 0.0504 | Train Acc: 0.9832 | F1: 0.9832 || Val Acc: 0.8770 | Val F1: 0.8792\n",
      "Epoch 11/50 | Train Loss: 0.0459 | Train Acc: 0.9851 | F1: 0.9851 || Val Acc: 0.8895 | Val F1: 0.8909\n",
      "Epoch 12/50 | Train Loss: 0.0449 | Train Acc: 0.9872 | F1: 0.9872 || Val Acc: 0.8890 | Val F1: 0.8908\n",
      "Epoch 13/50 | Train Loss: 0.0336 | Train Acc: 0.9901 | F1: 0.9901 || Val Acc: 0.8970 | Val F1: 0.8977\n",
      "Epoch 14/50 | Train Loss: 0.0380 | Train Acc: 0.9894 | F1: 0.9894 || Val Acc: 0.8870 | Val F1: 0.8883\n",
      "Epoch 15/50 | Train Loss: 0.0332 | Train Acc: 0.9902 | F1: 0.9902 || Val Acc: 0.8945 | Val F1: 0.8953\n",
      "Epoch 16/50 | Train Loss: 0.0325 | Train Acc: 0.9907 | F1: 0.9907 || Val Acc: 0.9000 | Val F1: 0.9002\n",
      "Epoch 17/50 | Train Loss: 0.0272 | Train Acc: 0.9926 | F1: 0.9926 || Val Acc: 0.8975 | Val F1: 0.8982\n",
      "Epoch 18/50 | Train Loss: 0.0290 | Train Acc: 0.9916 | F1: 0.9916 || Val Acc: 0.8955 | Val F1: 0.8957\n",
      "Epoch 19/50 | Train Loss: 0.0262 | Train Acc: 0.9919 | F1: 0.9919 || Val Acc: 0.8905 | Val F1: 0.8904\n",
      "Epoch 20/50 | Train Loss: 0.0299 | Train Acc: 0.9910 | F1: 0.9910 || Val Acc: 0.8930 | Val F1: 0.8939\n",
      "Epoch 21/50 | Train Loss: 0.0296 | Train Acc: 0.9917 | F1: 0.9917 || Val Acc: 0.9010 | Val F1: 0.9012\n",
      "Epoch 22/50 | Train Loss: 0.0280 | Train Acc: 0.9918 | F1: 0.9917 || Val Acc: 0.8950 | Val F1: 0.8956\n",
      "Epoch 23/50 | Train Loss: 0.0248 | Train Acc: 0.9934 | F1: 0.9934 || Val Acc: 0.9005 | Val F1: 0.9004\n",
      "Epoch 24/50 | Train Loss: 0.0254 | Train Acc: 0.9924 | F1: 0.9924 || Val Acc: 0.8965 | Val F1: 0.8964\n",
      "Epoch 25/50 | Train Loss: 0.0210 | Train Acc: 0.9938 | F1: 0.9937 || Val Acc: 0.9005 | Val F1: 0.9004\n",
      "Epoch 26/50 | Train Loss: 0.0210 | Train Acc: 0.9938 | F1: 0.9938 || Val Acc: 0.8985 | Val F1: 0.8981\n",
      "Epoch 27/50 | Train Loss: 0.0203 | Train Acc: 0.9939 | F1: 0.9939 || Val Acc: 0.9000 | Val F1: 0.8992\n",
      "Epoch 28/50 | Train Loss: 0.0189 | Train Acc: 0.9940 | F1: 0.9940 || Val Acc: 0.8970 | Val F1: 0.8959\n",
      "Epoch 29/50 | Train Loss: 0.0230 | Train Acc: 0.9940 | F1: 0.9940 || Val Acc: 0.8970 | Val F1: 0.8966\n",
      "Epoch 30/50 | Train Loss: 0.0208 | Train Acc: 0.9931 | F1: 0.9931 || Val Acc: 0.8935 | Val F1: 0.8925\n",
      "Epoch 31/50 | Train Loss: 0.0225 | Train Acc: 0.9932 | F1: 0.9932 || Val Acc: 0.8990 | Val F1: 0.8986\n",
      "Epoch 32/50 | Train Loss: 0.0188 | Train Acc: 0.9939 | F1: 0.9939 || Val Acc: 0.8995 | Val F1: 0.8992\n",
      "Epoch 33/50 | Train Loss: 0.0151 | Train Acc: 0.9952 | F1: 0.9952 || Val Acc: 0.9025 | Val F1: 0.9021\n",
      "Epoch 34/50 | Train Loss: 0.0186 | Train Acc: 0.9943 | F1: 0.9943 || Val Acc: 0.9055 | Val F1: 0.9052\n",
      "Epoch 35/50 | Train Loss: 0.0159 | Train Acc: 0.9946 | F1: 0.9946 || Val Acc: 0.9065 | Val F1: 0.9060\n",
      "Epoch 36/50 | Train Loss: 0.0146 | Train Acc: 0.9952 | F1: 0.9953 || Val Acc: 0.9050 | Val F1: 0.9049\n",
      "Epoch 37/50 | Train Loss: 0.0173 | Train Acc: 0.9948 | F1: 0.9948 || Val Acc: 0.8990 | Val F1: 0.8986\n",
      "Epoch 38/50 | Train Loss: 0.0164 | Train Acc: 0.9947 | F1: 0.9947 || Val Acc: 0.9010 | Val F1: 0.9007\n",
      "Epoch 39/50 | Train Loss: 0.0161 | Train Acc: 0.9952 | F1: 0.9952 || Val Acc: 0.8950 | Val F1: 0.8945\n",
      "Epoch 40/50 | Train Loss: 0.0227 | Train Acc: 0.9932 | F1: 0.9932 || Val Acc: 0.9045 | Val F1: 0.9041\n",
      "Epoch 41/50 | Train Loss: 0.0184 | Train Acc: 0.9943 | F1: 0.9943 || Val Acc: 0.9040 | Val F1: 0.9044\n",
      "Epoch 42/50 | Train Loss: 0.0134 | Train Acc: 0.9953 | F1: 0.9953 || Val Acc: 0.9030 | Val F1: 0.9033\n",
      "Epoch 43/50 | Train Loss: 0.0123 | Train Acc: 0.9959 | F1: 0.9959 || Val Acc: 0.9040 | Val F1: 0.9039\n",
      "Epoch 44/50 | Train Loss: 0.0109 | Train Acc: 0.9962 | F1: 0.9962 || Val Acc: 0.8985 | Val F1: 0.8990\n",
      "Epoch 45/50 | Train Loss: 0.0139 | Train Acc: 0.9954 | F1: 0.9954 || Val Acc: 0.9015 | Val F1: 0.9014\n",
      "Epoch 46/50 | Train Loss: 0.0183 | Train Acc: 0.9941 | F1: 0.9941 || Val Acc: 0.9005 | Val F1: 0.8999\n",
      "Epoch 47/50 | Train Loss: 0.0137 | Train Acc: 0.9953 | F1: 0.9953 || Val Acc: 0.9015 | Val F1: 0.9012\n",
      "Epoch 48/50 | Train Loss: 0.0131 | Train Acc: 0.9957 | F1: 0.9957 || Val Acc: 0.9035 | Val F1: 0.9037\n",
      "Epoch 49/50 | Train Loss: 0.0095 | Train Acc: 0.9966 | F1: 0.9966 || Val Acc: 0.9020 | Val F1: 0.9018\n",
      "Epoch 50/50 | Train Loss: 0.0113 | Train Acc: 0.9959 | F1: 0.9959 || Val Acc: 0.9010 | Val F1: 0.9009\n"
     ]
    }
   ],
   "source": [
    "# Parameters tuning\n",
    "model = CNN_LSTM_Ensemble(\n",
    "    vocab_size=len(word2idx),        # vocabulary size\n",
    "    embedding_dim=100,               # Dimensions of word vectors\n",
    "    hidden_dim=128,                  # Hidden layer dimensions\n",
    "    output_dim=6,                    # The number of output categories\n",
    "    padding_idx=word2idx.get(\"[PAD]\", 0)  # Index of pad token\n",
    ")\n",
    "train_instances = preprocess(dataset[\"train\"], tokenizer)\n",
    "val_instances = preprocess(dataset[\"validation\"], tokenizer)\n",
    "\n",
    "train_batches = batching_lstm(train_instances, batch_size=32, shuffle=True)\n",
    "val_batches = batching_lstm(val_instances, batch_size=32, shuffle=False)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_and_evaluate(model, train_batches, val_batches, num_epochs=50, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07e1abf2-6dcd-4a3d-8643-e21b5c664af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.93      0.92      0.92       550\n",
      "         joy       0.93      0.93      0.93       704\n",
      "        love       0.83      0.85      0.84       178\n",
      "       anger       0.89      0.93      0.91       275\n",
      "        fear       0.85      0.83      0.84       212\n",
      "    surprise       0.83      0.78      0.80        81\n",
      "\n",
      "    accuracy                           0.90      2000\n",
      "   macro avg       0.87      0.87      0.87      2000\n",
      "weighted avg       0.90      0.90      0.90      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get the label name order\n",
    "label_names = dataset[\"train\"].features[\"label\"].names  # ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "\n",
    "# Model prediction validation set\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in val_batches:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model(x_batch)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        true_labels.extend(y_batch.cpu().numpy())\n",
    "        pred_labels.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Print each category's precision, recall, f1-score\n",
    "print(classification_report(true_labels, pred_labels, target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54d764f3-a6e6-4f97-8813-dc7be4bebad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported: True\n",
      "CUDA version: 11.8\n",
      "Current device: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA is supported:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Current device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
