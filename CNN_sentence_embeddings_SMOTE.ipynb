{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d018716b-0cb0-43f3-abfd-ace6f613534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from tokenizers import Tokenizer, models, trainers\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e5d4d27-6fcc-4e38-ad99-5aedad8842fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = load_dataset(\"dair-ai/emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0315fef-7e42-4278-8dd5-6078ec8b754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c66816cb-e29d-4940-95bc-b785747447e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = emotions[\"train\"]\n",
    "validation_data = emotions[\"validation\"]\n",
    "test_data = emotions[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b2a4d5-ea1c-4196-844c-fddd2a6f2259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "vocab_n = 5000\n",
    "sequence_len = 64\n",
    "\n",
    "# Initialize a tokenizer using BPE (Byte Pair Encoding)\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.enable_padding(length=sequence_len)\n",
    "tokenizer.enable_truncation(max_length=sequence_len)\n",
    "tokenizer_trainer = trainers.BpeTrainer(vocab_size=vocab_n)\n",
    "tokenizer.train_from_iterator(train_data[\"text\"], trainer=tokenizer_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfa331a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Batches: 100%|██████████| 500/500 [00:04<00:00, 123.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After SMOTE class distribution: {np.int64(0): np.int64(5362), np.int64(1): np.int64(5362), np.int64(2): np.int64(5362), np.int64(3): np.int64(5362), np.int64(4): np.int64(5362), np.int64(5): np.int64(5362)}\n"
     ]
    }
   ],
   "source": [
    "# Load a small and fast transformer model for sentence embeddings\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Generate sentence embeddings for the training texts\n",
    "X_embeddings = model.encode(train_data[\"text\"], show_progress_bar=True)\n",
    "\n",
    "# Apply SMOTE on sentence embeddings\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_embeddings, train_data[\"label\"])\n",
    "\n",
    "# Convert to torch tensors for use in training\n",
    "X_tensor = torch.tensor(X_resampled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_resampled, dtype=torch.long)\n",
    "\n",
    "print(\"After SMOTE class distribution:\", dict(zip(*np.unique(y_resampled, return_counts=True))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8b3d861-7e4f-48d3-b643-06e9b9a88405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, tokenizer: Tokenizer):\n",
    "    \"\"\" \n",
    "    Helper function to tokenize text and return corresponding token IDs as tensors.\n",
    "\n",
    "    Args:\n",
    "        text, str: Text instance from training data.\n",
    "        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.\n",
    "    Returns:\n",
    "        Tensor: One-dimensional PyTorch tensor with token IDs.\n",
    "    \"\"\"\n",
    "    return torch.tensor(tokenizer.encode(text).ids)\n",
    "\n",
    "\n",
    "def preprocess_label(label: int):\n",
    "    \"\"\" \n",
    "    Helper function to return label as tensor.\n",
    "\n",
    "    Args:\n",
    "        label, int: Label from instance.\n",
    "    Returns:\n",
    "        Tensor: One-dimensional PyTorch tensor containing the label index.\n",
    "    \"\"\"\n",
    "    return torch.tensor(label)\n",
    "\n",
    "\n",
    "def preprocess(data: dict, tokenizer: Tokenizer):\n",
    "    \"\"\" \n",
    "    Transforms input dataset to tokenized vector representations.\n",
    "\n",
    "    Args:\n",
    "        data, dict: Dictionary with text instances and labels.\n",
    "        tokenizer, Tokenizer: The respective tokenizer to be used for tokenization.\n",
    "    Returns:\n",
    "        list: List with tensors for the input texts and labels.\n",
    "    \"\"\"\n",
    "    instances = []\n",
    "\n",
    "    for text, label in zip(data[\"text\"], data[\"label\"]):\n",
    "        input = preprocess_text(text, tokenizer)\n",
    "        label = preprocess_label(label)\n",
    "        \n",
    "        instances.append((input, label))\n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85779c8a-cb93-472c-8e61-8e7aadf46fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instances = preprocess(train_data, tokenizer)\n",
    "val_instances = preprocess(validation_data, tokenizer)\n",
    "test_instances = preprocess(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba01debf-56ad-4302-aa85-8b506e686a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching\n",
    "\n",
    "def batching(instances: list, batch_size: int, shuffle: bool):\n",
    "    \"\"\" \n",
    "    Batches input instances along the given size and returns list of batches.\n",
    "\n",
    "    Args:\n",
    "        instances, list: List of instances, containing a tuple of two tensors \n",
    "            for each text as well as corresponding label.\n",
    "        batch_size, int: Size for batches.\n",
    "        shuffle, bool: If true, the instances will be shuffled before batching.\n",
    "    Returns:\n",
    "        list: List containing tuples that correspond to single batches.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        random.shuffle(instances)\n",
    "\n",
    "    batches = []\n",
    "\n",
    "    # We iterate through the instances with batch_size steps\n",
    "    for i in range(0, len(instances), batch_size):\n",
    "\n",
    "        # Stacking the instances with dim=0 (default value)\n",
    "        batch_texts = torch.stack(\n",
    "            [instance[0] for instance in instances[i : i + batch_size]]\n",
    "        )\n",
    "        batch_labels = torch.stack(\n",
    "            [instance[1] for instance in instances[i : i + batch_size]]\n",
    "        )\n",
    "\n",
    "        batches.append((batch_texts, batch_labels))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5aa7e1e-9735-4c8d-8761-68d251b2036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Network\n",
    "\n",
    "class CNN_Classifier(nn.Module):\n",
    "    \"\"\" \n",
    "    CNN for sentiment classification with 6 classes, consisting of an embedding \n",
    "    layer, two convolutional layers with different filter sizes, different \n",
    "    pooling sizes, as well as one linear output layer.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # We can implement embeddings as a simple lookup-table for given word \n",
    "        # indices\n",
    "        self.embedding = nn.Embedding(tokenizer.get_vocab_size(), 300)\n",
    "\n",
    "        # One-dimensional convolution-layer with 300 input channels, and 100  \n",
    "        # output channels as well as kernel size of 3; note that the\n",
    "        # one-dimensional convolutional layer has 3 dimensions\n",
    "        self.conv_1 = nn.Conv1d(300, 100, 3, padding=\"same\")\n",
    "\n",
    "        # Pooling with with a one-dimensional sliding window of length 3, \n",
    "        # reducing in this fashion the sequence length \n",
    "        self.pool_1 = nn.MaxPool1d(3)\n",
    "\n",
    "        # The input will be the reduced number of maximum picks from the\n",
    "        # previous operation; the dimension of those picks is the same as the\n",
    "        # output channel size from self.conv_1. We apply a different filter of \n",
    "        # size 5.\n",
    "        self.conv_2 = nn.Conv1d(100, 50, 5, padding=\"same\")\n",
    "\n",
    "        # Pooling with window size of 5\n",
    "        self.pool_2 = nn.MaxPool1d(5)\n",
    "\n",
    "        # Final fully connected linear layer from the 50 output channels to the\n",
    "        # 6 sentiment categories \n",
    "        self.linear_layer = nn.Linear(50, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Defining the forward pass of an input batch x.\n",
    "\n",
    "        Args:\n",
    "            x, tensor: The input is a batch of tweets from the data.\n",
    "        Returns:\n",
    "            y, float: The output are the logits from the final layer.\n",
    "        \"\"\"\n",
    "        # x will correspond here to a batch; therefore, the input dimensions of \n",
    "        # the embedding will be by PyTorch convention as follows:\n",
    "        # [batch_size, seq_len, emb_dim]\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Unfortunately the embedding tensor does not correspond to the shape \n",
    "        # that is needed for nn.Conv1d(); for this reason, we must switch its \n",
    "        # order to [batch_size, emb_dim, seq_len] for PyTorch\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # We can wrap the ReLu activation function around our convolution layer\n",
    "        # The output tensor will have the following shape: \n",
    "        # [batch_size, 100, seq_len]\n",
    "        x = nn.functional.relu(self.conv_1(x))\n",
    "\n",
    "        # Applying max pooling of size 3 means that the output length of the \n",
    "        # sequence is shrunk to seq_len//3\n",
    "        x = self.pool_1(x)\n",
    "\n",
    "        # Output of the following layer: [batch_size, 50, seq_len//3]\n",
    "        x = nn.functional.relu(self.conv_2(x))\n",
    "\n",
    "        # Shrinking the sequence length by 5\n",
    "        x = self.pool_2(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # At this point we have a tensor with 3 dimensions; however, the final layer \n",
    "        # requires an input of size [batch_size x 50]. To get this value we can \n",
    "        # aggregate the values and continue only with their mean\n",
    "        x = x.mean(dim=-1)\n",
    "\n",
    "        # In this fasion, the linear layer can be used to make predictions\n",
    "        y = self.linear_layer(x)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def fit(self, train_instances, val_instances, epochs, batch_size):\n",
    "        \"\"\" \n",
    "        Gradient based fitting method with Adam optimization and automatic \n",
    "        evaluation (F1 score) for each epoch.\n",
    "\n",
    "        Args:\n",
    "            train_instances, list: List of instance tuples.\n",
    "            val_instances, list: List of instance tuples.\n",
    "            epochs, int: Number of training epochs.\n",
    "            batch_size, int: Number of batch size.\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_batches = batching(\n",
    "                train_instances,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True)\n",
    "            \n",
    "            for inputs, labels in tqdm(train_batches):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = nn.functional.cross_entropy(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            train_f1 = self.evaluate2(train_instances, batch_size=batch_size)\n",
    "            val_f1 = self.evaluate2(val_instances, batch_size=batch_size)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1} train F1 score: {train_f1}, validation F1 score: {val_f1}\")\n",
    "\n",
    "    def predict(self, input):\n",
    "        \"\"\" \n",
    "        To make inferences from the model.\n",
    "\n",
    "        Args:\n",
    "            input, tensor: Single instance.\n",
    "        Returns:\n",
    "            int: Integer for most probable class.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        outputs = self(input)\n",
    "\n",
    "        return torch.argmax(outputs, dim=-1)\n",
    "\n",
    "    def evaluate(self, instances, batch_size):\n",
    "        \"\"\" \n",
    "        To evaluate model's performance by various processes/standard.\n",
    "\n",
    "        Args:\n",
    "            instances, list: List of instance tuples.\n",
    "            batch_size, int: Batch size.\n",
    "        Returns:\n",
    "            float: Macro F1 score for given instances.\n",
    "        \"\"\"\n",
    "        batches = batching(instances, batch_size=batch_size, shuffle=False)\n",
    "        y_test = []\n",
    "        y_pred = []\n",
    "\n",
    "        for inputs, labels in batches:\n",
    "            y_test.extend(labels)\n",
    "            y_pred.extend(self.predict(inputs))\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='macro')\n",
    "        recall = recall_score(y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        print(\"CNN Classifier:\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1 Score: {f1}\")\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "        print(f\"Classification Report:\\n{report}\")\n",
    "    \n",
    "    def evaluate2(self, instances, batch_size):\n",
    "        \"\"\" \n",
    "        To make evaluations against the gold standard (true labels) from the \n",
    "        data.\n",
    "\n",
    "        Args:\n",
    "            instances, list: List of instance tuples.\n",
    "            batch_size, int: Batch size.\n",
    "        Returns:\n",
    "            float: Macro F1 score for given instances.\n",
    "        \"\"\"\n",
    "        batches = batching(instances, batch_size=batch_size, shuffle=False)\n",
    "        true = []\n",
    "        pred = []\n",
    "\n",
    "        for inputs, labels in batches:\n",
    "            true.extend(labels)\n",
    "            pred.extend(self.predict(inputs))\n",
    "\n",
    "        return f1_score(true, pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf02f634-df1b-498f-a9e2-b46b89ba98a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 273.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train F1 score: 0.5840321467773238, validation F1 score: 0.5377111231942294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 258.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train F1 score: 0.9050560351203166, validation F1 score: 0.8365100276197261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 255.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train F1 score: 0.9613028281910769, validation F1 score: 0.8645149751730302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 253.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train F1 score: 0.9786472144147581, validation F1 score: 0.8673525036939628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 238.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train F1 score: 0.9878496023146632, validation F1 score: 0.8694417226393752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 254.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train F1 score: 0.9870641162103458, validation F1 score: 0.8594942471862733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 254.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train F1 score: 0.9888874873001031, validation F1 score: 0.8571500915776241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 256.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train F1 score: 0.9918405098529431, validation F1 score: 0.8588954133980483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 268.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train F1 score: 0.9908891570732058, validation F1 score: 0.8639545599047805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 268.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 train F1 score: 0.9933817278344823, validation F1 score: 0.8564615767109344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 252.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 train F1 score: 0.9949796594581833, validation F1 score: 0.870765449584288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 247.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 train F1 score: 0.9942951235572225, validation F1 score: 0.8643489480998453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 256.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 train F1 score: 0.9919676054559731, validation F1 score: 0.8541470313276807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 253.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 train F1 score: 0.9944790660587429, validation F1 score: 0.867018533106604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 257.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 train F1 score: 0.9881898027772054, validation F1 score: 0.8536122854355829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 257.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 train F1 score: 0.9927611772552573, validation F1 score: 0.8548240288292498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 261.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 train F1 score: 0.9959265512967602, validation F1 score: 0.8709424518706799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 263.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 train F1 score: 0.996051155601941, validation F1 score: 0.8709094870506715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 266.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 train F1 score: 0.9960687531819308, validation F1 score: 0.8649695208685927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 250.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 train F1 score: 0.9953819485649739, validation F1 score: 0.8649127475143875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 263.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 train F1 score: 0.9949001192547658, validation F1 score: 0.8607965519014388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 271.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 train F1 score: 0.9961821549176739, validation F1 score: 0.8751131339353672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 268.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 train F1 score: 0.9952998434606041, validation F1 score: 0.8722814643208142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 260.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 train F1 score: 0.9959289916894402, validation F1 score: 0.8673770114273985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 265.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 train F1 score: 0.99505339143732, validation F1 score: 0.8570339816766998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 258.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 train F1 score: 0.9938829321279562, validation F1 score: 0.8629106631575051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 254.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 train F1 score: 0.9961773042107609, validation F1 score: 0.871163579144575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 263.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 train F1 score: 0.996762771627086, validation F1 score: 0.868653545375674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 266.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 train F1 score: 0.9969566753622373, validation F1 score: 0.8673296595292573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 252.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 train F1 score: 0.9932782704274903, validation F1 score: 0.8599982005455981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 264.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 train F1 score: 0.9965000086954842, validation F1 score: 0.8682760617192485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 262.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 train F1 score: 0.9954772724120097, validation F1 score: 0.8621542229081611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 263.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 train F1 score: 0.9966203049206568, validation F1 score: 0.855171565235405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 256.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 train F1 score: 0.9963607488750651, validation F1 score: 0.8552176568398254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 262.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 train F1 score: 0.995205728730542, validation F1 score: 0.8604900424224965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 263.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 train F1 score: 0.9948216335301702, validation F1 score: 0.8658225058991226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 265.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 train F1 score: 0.9951865895909102, validation F1 score: 0.8682974261665484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 262.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 train F1 score: 0.9966460110636985, validation F1 score: 0.862593854400253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 263.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 train F1 score: 0.9965167192028926, validation F1 score: 0.8641331129425177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 265.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 train F1 score: 0.9933028773441065, validation F1 score: 0.8652833212643394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 264.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 train F1 score: 0.9964453948052205, validation F1 score: 0.8728728313498055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 257.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 train F1 score: 0.9951471013050465, validation F1 score: 0.8654611949524087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 264.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 train F1 score: 0.9945664472878116, validation F1 score: 0.8635127287208743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 262.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 train F1 score: 0.996715064914178, validation F1 score: 0.8602342917983141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 262.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 train F1 score: 0.9969477435517954, validation F1 score: 0.8582006948062414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 262.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 train F1 score: 0.9968720556483932, validation F1 score: 0.8652546550746525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 258.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 train F1 score: 0.9967606738685437, validation F1 score: 0.8653764409039338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 265.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 train F1 score: 0.9969552809416294, validation F1 score: 0.8598880813855573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 249.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 train F1 score: 0.991440676892807, validation F1 score: 0.8478531742387924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 252.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 train F1 score: 0.9942998287670823, validation F1 score: 0.8743280720529613\n"
     ]
    }
   ],
   "source": [
    "classifier = CNN_Classifier()\n",
    "classifier.fit(train_instances, val_instances, epochs=50, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3b6ec90-a70f-4fb7-b02c-1261f9d563c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Classifier:\n",
      "Accuracy: 0.8915\n",
      "Precision: 0.8382884144946093\n",
      "Recall: 0.8591981982533056\n",
      "F1 Score: 0.8438163876927041\n",
      "Confusion Matrix:\n",
      "[[557   8   3  10   3   0]\n",
      " [  7 636  39   5   3   5]\n",
      " [  5  25 124   2   0   3]\n",
      " [ 14  12   3 240   5   1]\n",
      " [ 17   1   2  12 168  24]\n",
      " [  1   3   0   1   3  58]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94       581\n",
      "           1       0.93      0.92      0.92       695\n",
      "           2       0.73      0.78      0.75       159\n",
      "           3       0.89      0.87      0.88       275\n",
      "           4       0.92      0.75      0.83       224\n",
      "           5       0.64      0.88      0.74        66\n",
      "\n",
      "    accuracy                           0.89      2000\n",
      "   macro avg       0.84      0.86      0.84      2000\n",
      "weighted avg       0.90      0.89      0.89      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1_test = classifier.evaluate(test_instances, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec18f2b-5ba1-466b-9442-fd2f2cdc8be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
